<!doctype html><html class="no-js" lang="en"><head><meta charset="utf-8"><meta name="author" content="Geshan Manandhar"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="Learn how to use Ollama and Open WebUI inside Docker with Docker compose to run any open LLM and create your own mini ChatGPT."><meta name="keywords" content="ollama docker, ollama docker compose, ollama open webui, ollama docker webui, ollama docker compose webui"><meta name="p:domain_verify" content="e654c68562abebfa25c291f59d7d00e8"><meta property="og:type" content="website"><meta property="og:url" content="https://geshan.com.np/blog/2025/02/ollama-docker-compose/"><meta property="og:title" content="How to use Ollama and Open WebUI with Docker Compose [Part 4]"><meta property="og:description" content="Learn how to use Ollama and Open WebUI inside Docker with Docker compose to run any open LLM and create your own mini ChatGPT."><meta property="og:site_name" content="Geshan&#39;s Blog"><meta property="og:image" content="https://geshan.com.np/images/ollama-docker-compose/01ollama-docker-compose.jpg"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:widgets:new-embed-design" content="on"><meta name="twitter:site" content="@geshan"><meta name="twitter:creator" content="@geshan"><meta name="twitter:title" content="How to use Ollama and Open WebUI with Docker Compose [Part 4]"><meta name="twitter:description" content="Learn how to use Ollama and Open WebUI inside Docker with Docker compose to run any open LLM and create your own mini ChatGPT."><meta name="twitter:image:src" content="https://geshan.com.np/images/ollama-docker-compose/01ollama-docker-compose.jpg"><link rel="canonical" href="https://geshan.com.np/blog/2025/02/ollama-docker-compose/"><meta property="fb:pages" content="30717799226"><meta property="fb:app_id" content="106030259434380"><meta name="monetization" content="$ilp.uphold.com/aKHWpqhphm9f"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicons/favicon-16x16.png"><link href="/atom.xml" rel="alternate" title="Geshan&#39;s Blog" type="application/atom+xml"><title>How to use Ollama with Open WebUI with Docker and Docker Compose</title><link rel="preconnect" href="/" crossorigin><link rel="preload" href="/css/fonts.css" as="style"><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-500-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-pro/source-sans-pro-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="/css/fonts.css"><link rel="stylesheet" href="/css/tw-006.css"><link rel="alternate" href="/atom.xml" type="application/atom+xml" title="Geshan&#39;s Blog"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-P3NXCVQEPE"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-P3NXCVQEPE');</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWT2D9T');</script><script src="https://cdn.jsdelivr.net/npm/@statsig/js-client@3/build/statsig-js-client+session-replay+web-analytics.min.js?apikey=client-rYO7rX8WSUyyLg9pKJwymLxM71tCE0CKgxgtUj4akzK"></script><link rel="manifest" href="/manifest.json"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="application-name" content="Geshan.com.np"><meta name="apple-mobile-web-app-title" content="Geshan.com.np"><meta name="msapplication-starturl" content="/index.html"><meta name="theme-color" content="#6947E7"><script>if ("serviceWorker" in navigator) {
    navigator.serviceWorker.register("/sw.js").then(function(registration) {
        console.log('ServiceWorker registration successful with scope: ', registration.scope);
    }, function(err) {
        console.log('ServiceWorker registration failed: ', err);
    });
  }</script></head><body class="overflow-x-hidden font-ui flex flex-col min-h-screen"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWT2D9T" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><header role="banner"><div><nav class="bg-white" role="navigation"><div class="w-full md:max-w-6xl mx-auto py-4 px-4"><div class="flex items-center justify-between h-16"><div class="flex-shrink-0"><a href="/" class="flex items-center"><img class="h-12 w-12" src="/images/theme/new_logo.svg" alt="Geshan G"></a></div><div class="flex items-center gap-12"><div class="hidden sm:flex items-center gap-12"><a href="/posts/1/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Posts </a><a href="/about/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Newsletter</a></div><form class="search flex items-center" action="https://www.google.com/search" method="GET"><input type="hidden" name="sitesearch" value="geshan.com.np"><div class="relative"><input class="bg-gray-100 border border-neutralGray rounded-lg py-2 pl-10 pr-4 font-body text-sm focus:outline-none focus:ring-2 focus:ring-avocado focus:border-transparent w-40" type="text" name="q" placeholder="Search" label="search"> <svg class="absolute left-3 top-1/2 transform -translate-y-1/2 w-4 h-4 text-neutralGray" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/></svg></div></form><button type="button" class="sm:hidden inline-flex items-center justify-center p-2 rounded-md text-black focus:outline-none focus:ring-2 focus:ring-inset focus:ring-avocado" onclick="mobileView()" aria-controls="mobile-menu" aria-expanded="false"><span class="sr-only">Open main menu</span> <svg class="w-6 h-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button></div></div></div><div class="mobile-menu fixed inset-0 z-40 hidden sm:hidden bg-black bg-opacity-40" role="dialog" aria-modal="true"><div class="absolute inset-0" onclick="mobileView()" aria-hidden="true"></div><div class="mobile-menu-panel relative ml-auto flex h-full w-10/12 max-w-sm flex-col justify-start bg-white shadow-xl"><div class="flex items-center justify-between px-4 py-4 border-b border-gray-200"><div class="flex items-center gap-3"><img class="h-8 w-8" src="/images/theme/new_logo.svg" alt="Geshan G"> <span class="text-lg font-semibold text-textColor font-body">Geshan's Blog</span></div><button type="button" class="inline-flex items-center justify-center rounded-full p-2 text-gray-700 hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-avocado" onclick="mobileView()" aria-label="Close main menu"><svg class="h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg></button></div><nav class="overflow-y-auto px-4 py-6 space-y-2"><a href="/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Home </a><a href="/posts/1/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Posts </a><a href="/about/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Newsletter</a></nav><div class="px 4 py-4 border-t border-gray-200"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="rounded-lg bg-darkAvocado px-4 py-2 text-base font-regular font-body text-white hover:bg-avocado hover:text-black transition-colors w-2/3 mx-auto ml-4">Connect on LinkedIn</a></div></div></div></nav></div></header><main id="wrap" role="main" class="flex-grow md:px-0"><div id="content"><div class="row"><div><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Geshan&#39;s Blog",
    "alternativeHeadline": "How to use Ollama and Open WebUI with Docker Compose [Part 4]",
    "image": "https://geshan.com.np/images/ollama-docker-compose/01ollama-docker-compose.jpg",
    "editor": "Geshan Manandhar",
    "genre": "AI",
    "keywords": "ollama docker, ollama docker compose, ollama open webui, ollama docker webui, ollama docker compose webui",
    "url": "https://geshan.com.np/blog/2025/02/ollama-docker-compose/",
    "datePublished": "2025-02-11",
    "dateCreated": "2025-02-11",
    "dateModified": "2025-02-11",
    "description": "Learn how to use Ollama and Open WebUI inside Docker with Docker compose to run any open LLM and create your own mini ChatGPT.",
    "articleBody": "Ollama gives you one of the easiest ways to run most open LLMs on your machine. It is open-source and easy to use. In addition to using it with a command line or its APIs, you can use it with a web user interface using Open WebUI. This post will teach you how to run Ollama and Open WebUI to run any open LLM with a web-based chat interface like ChatGPT. Let’s get started!Table of contents #Recap of the Ollama seriesPrerequisitesOpen WebUIOllama Docker ComposeOllama Open WebUI Docker ServicesRunning Ollama and Open WebUI with Docker composeThe docker images are hugeHosting Ollama on the cloudConclusionRecap of the Ollama series #This is part 4 of the Ollama blog series. In the first part, you learned what an Ollama is, its features, and how to run it on your local machine.The second part delved into the Ollama commands you can execute on the CLI. Part 3 of the series shed light on some of the important Ollama APIs focusing on the  some code  icon at the top right of the screen. Parameters like temperature, top K, Top P, and others can be changed on the Open WebUI configs as follows:You now have your mini chatGPT running locally. Since it is Ollama and the model has been downloaded, it can run even without the internet on a plane. Depending on the resources available, such as disk space, CPU/GPU, and memory, you can download other models, such as Llama, Microsoft Phi, Gemma 2, or DeepSeek, from Ollama’s model registry.The docker images are huge #The uncompressed Docker image for Ollama is 4.5 GB, which will grow bigger when you download a model. Similarly, the uncompressed image for Open WebUI is 3.77 GB. Both of them are huge, as you can see below:Make sure to have at least 9-10 GB of free space on your hard disk before downloading these large Docker images.Hosting Ollama on the cloud #You can follow this step-by-step tutorial to run Ollama on Google Cloud Run. If you are looking for a more production-ready Ollama docker image with a model (Gemma 2:9b) already pulled, have a look at this Dockerfile. You can easily change the version of Ollama and also download another model of your choice to host it on Google Cloud Run in serverless containers. You can follow this codelab to create a multi-container Cloud Run service with Ollama and Open WebUI together on Google Cloud Run where Open WebUI is the main pod (ingress frontend) and Ollama is a sidecar.As it is a Docker container, you can also run it on your Kubernetes cluster on GKE.Conclusion #This blog post explains how to run Ollama and Open WebUI with Docker Compose. Ollama is an open-source tool for running large language models (LLMs) on your machine, and Open WebUI provides a web-based chat interface for interacting with the models.The blog post first recaps the Ollama blog series and lists the prerequisites for running Ollama and Open WebUI with Docker Compose. It then explains Open WebUI and provides the Ollama Docker Compose file. Next, it explains the Ollama Open WebUI Docker services and how to run them with Docker Compose. It also notes that the Docker images are large, and the post provides guidance for hosting Ollama on the cloud. Keep exploring!",
    "author": { "@type": "Person", "name": "Geshan Manandhar" },
    "publisher": {
      "@type": "Organization",
      "name": "Geshan Manandhar",
      "logo": { "@type": "ImageObject", "url": "https://geshan.com.np/images/favicons/favicon-32x32.png" }
    },
    "mainEntityOfPage": { "@type": "WebPage", "@id": "https://geshan.com.np/blog/2025/02/ollama-docker-compose/" }
  }</script><div class="progress-bar"></div><div class="max-w-6xl py-3 mx-auto grid-cols-12 grid gap-15 px-4 md:px-0 mb-20"><div class="col-span-12"><article class="md:w-full mx-auto"><header class="page-header text-center"><h1 class="font-semibold font-heading text-gray text-center mb-6 tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to use Ollama and Open WebUI with Docker Compose [Part 4]</h1><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2025-02-11" data-updated="true"><time class="entry-date" datetime="2025-02-11"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">11-Feb-2025 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">11 MIN READ</span></div><button type="button" id="share-button" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIcons(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container" class="share-icons print:hidden hidden" aria-label="Share this post"><div class="flex items-center justify-center gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=How to use Ollama and Open WebUI with Docker Compose [Part 4] - &url=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></header><div class="entry-content clearfix font-body text-xl text-gray"><p>Ollama gives you one of the easiest ways to run most open LLMs on your machine. It is open-source and easy to use. In addition to using it with a command line or its APIs, you can use it with a web user interface using <a href="https://openwebui.com/">Open WebUI</a>. This post will teach you how to run Ollama and Open WebUI to run any open LLM with a web-based chat interface like ChatGPT. Let’s get started!</p><img class="center" src="/images/ollama-docker-compose/01ollama-docker-compose.jpg" title="How to use Ollama and Open WebUI with Docker Compose [Part 4]" alt="How to use Ollama and Open WebUI with Docker Compose [Part 4]"><h2 id="table-of-contents" tabindex="-1">Table of contents <a class="direct-link" href="#table-of-contents">#</a></h2><ul><li><a href="#recap-of-the-ollama-series">Recap of the Ollama series</a></li><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#open-webui">Open WebUI</a></li><li><a href="#ollama-docker-compose">Ollama Docker Compose</a><ul><li><a href="#ollama-open-webui-docker-services">Ollama Open WebUI Docker Services</a></li></ul></li><li><a href="#running-ollama-and-open-webui-with-docker-compose">Running Ollama and Open WebUI with Docker compose</a></li><li><a href="#the-docker-images-are-huge">The docker images are huge</a></li><li><a href="#hosting-ollama-on-the-cloud">Hosting Ollama on the cloud</a></li><li><a href="#conclusion">Conclusion</a></li></ul><h2 id="recap-of-the-ollama-series" tabindex="-1">Recap of the Ollama series <a class="direct-link" href="#recap-of-the-ollama-series">#</a></h2><p>This is part 4 of the Ollama blog series. In the <a href="/blog/2025/02/what-is-ollama/">first part</a>, you learned <a href="/blog/2025/02/what-is-ollama/#what-is-ollama">what an Ollama is</a>, its features, and how to run it on your local machine.</p><p>The second part delved into the <a href="/blog/2025/02/ollama-commands/">Ollama commands</a> you can execute on the CLI. Part 3 of the series shed light on some of the important <a href="/blog/2025/02/ollama-api/">Ollama APIs</a> focusing on the <code>generate</code> and <code>chat</code> endpoints.</p><p>This part involves running <a href="https://hub.docker.com/r/ollama/ollama">Ollama’s Docker image</a> and adding a web UI, the <a href="https://github.com/open-webui/open-webui">Open WebUI</a>, to provide a chat interface for any model Ollama can run. Like Ollama, Open WebUI is also open-source, with the code primarily in JavaScript, Python, and TypeScript. It also has a docker image pushed on the Google Container Registry, created from its <a href="https://github.com/open-webui/open-webui/blob/main/Dockerfile">Dockerfile</a>. You will use Docker Compose to run these two images together for a working application.</p><h2 id="prerequisites" tabindex="-1">Prerequisites <a class="direct-link" href="#prerequisites">#</a></h2><p>Before you start running some Docker Compose commands, be informed of some of the software that needs to be running on your machine:</p><ol><li>You will need Docker running on your machine, for this example, I am using Docker 27.4.0 on Mac</li><li>Make sure you have Docker Compose available as well (it used to be a different install when it was <code>docker-compose</code> when it was in v1, from <a href="https://stackoverflow.com/a/66516826">v2</a> it is coupled with the Docker Desktop installation). I am using Docker Compose version v2.31.0-desktop.2 on a Mac)</li><li>It would be good to know about Docker volumes, docker ports, and basic <a href="/blog/2022/05/docker-commands/">docker commands</a></li></ol><p>You can read the <a href="/blog/2024/04/docker-for-beginners/">Docker for beginners</a> tutorial for a refresher on Docker. Please read this <a href="/blog/2024/04/docker-compose-tutorial/">docker compose tutorial</a> to learn more about Docker Compose.</p><h2 id="open-webui" tabindex="-1">Open WebUI <a class="direct-link" href="#open-webui">#</a></h2><p>Open Web UI is a user interface for interacting with large language models. It offers a streamlined and intuitive way to communicate with and manage these models, making them more accessible and user-friendly.</p><p>Open Web UI aims to simplify working with large language models. It allows users to harness their power for various applications, including content creation, research, and software development.</p><h2 id="ollama-docker-compose" tabindex="-1">Ollama Docker Compose <a class="direct-link" href="#ollama-docker-compose">#</a></h2><p>The Docker images for both Ollama and Open WebUI are not small. Ollama’s latest (version 0.5.7 at the time of writing) is 4.76 GB uncompressed, and Open WebUI’s main tag is 3.77 GB uncompressed. Below is the <code>docker-compose.yaml</code> file that has both Ollama and Open Web UI:</p><pre class="language-bash"><code class="language-bash">services:<br>  ollama:<br>    image: ollama/ollama:latest<br>    ports:<br>      - <span class="token number">11434</span>:11434<br>    volumes:<br>      - ollama:/root/.ollama<br>    container_name: ollama<br>    tty: <span class="token boolean">true</span><br>    restart: unless-stopped<br><br>  open-webui:<br>    image: ghcr.io/open-webui/open-webui:main<br>    container_name: open-webui<br>    volumes:<br>      - open-webui:/app/backend/data<br>    depends_on:<br>      - ollama<br>    ports:<br>      - <span class="token number">3000</span>:8080<br>    environment:<br>      - <span class="token string">'OLLAMA_BASE_URL=http://ollama:11434'</span><br>      - <span class="token string">'WEBUI_SECRET_KEY='</span><br>    extra_hosts:<br>      - host.docker.internal:host-gateway<br>    restart: unless-stopped<br><br>volumes:<br>  ollama: <span class="token punctuation">{</span><span class="token punctuation">}</span><br>  open-webui: <span class="token punctuation">{</span><span class="token punctuation">}</span></code></pre><p>This file sets up your local environment to run any AI model (such as a large language model or LLM) and interact with it through a user-friendly web interface. It's like setting up a mini-cloud service on your machine.</p><p>This docker-compose.yml file sets up a two-part application:</p><ul><li><p>Ollama runs large language models (LLMs) locally on your computer. Think of it like the &quot;engine&quot; that powers the AI. It's like having your mini-ChatGPT running.</p></li><li><p>Open WebUI is a user-friendly web interface that allows you to interact with Ollama. It's like a dashboard that allows you to talk to the AI engine. It provides a nice visual way to send prompts and see responses.</p></li></ul><h3 id="ollama-open-webui-docker-services" tabindex="-1">Ollama Open WebUI Docker Services <a class="direct-link" href="#ollama-open-webui-docker-services">#</a></h3><p>Let’s look at the <code>services</code> section of the above <code>docker-compose.yaml</code> file:</p><p>Services is the main section where you define your application's different parts (containers). Each &quot;service&quot; is a separate program running in its isolated environment.</p><ul><li><p><code>ollama</code>: This defines the first service, named &quot;ollama&quot;.</p></li><li><p><code>image: ollama/ollama:latest</code>: This tells Docker which pre-built &quot;image&quot; to use. An image is like a template for a container. <code>ollama/ollama:lates</code>t means you using the official Ollama image, and <code>latest</code> means we want the most recent version.</p></li><li><p><code>ports: - 11434:11434</code>: This maps port 11434 on your host machine (your computer) to port 11434 inside the Ollama container. Ollama listens for requests on port 11434. This allows other applications (like Open WebUI) to talk to Ollama.</p></li><li><p><code>volumes</code>: - <code>ollama:/root/.ollama</code> creates a persistent storage area. <code>/root/.ollama</code> is where Ollama stores its data (like downloaded models). ollama: (defined at the bottom of the file) is a named volume. This means the data will persist even if you stop and restart the container. Without this, you'd lose all your downloaded models every time you stopped Ollama. It's like giving Ollama a dedicated hard drive that doesn't get erased.</p></li><li><p><code>container_name</code>: <code>ollama</code>: This gives the container a specific name, &quot;ollama,&quot; making it easier to refer to.</p></li><li><p><code>tty: true</code> allocates a pseudo-TTY, which can be helpful for interactive sessions. It helps the container handle input and output, making it behave more like a regular terminal. Programs that expect to interact with a user often need this.</p></li><li><p><code>restart: unless-stopped</code>: This tells Docker to automatically restart the Ollama container if it crashes or stops for any reason unless you explicitly stop it yourself (e.g., using docker compose down). It's like setting an auto-restart feature.</p></li><li><p>open-webui: This defines the second service, named &quot;open-webui&quot;.</p></li><li><p><code>image: ghcr.io/open-webui/open-webui:main</code>: This uses the Open WebUI image from the GitHub Container Registry (<a href="http://ghcr.io">ghcr.io</a>). <code>main</code> specifies a particular version (the main branch).</p></li><li><p><code>container_name: open-webui</code>: Gives the container a specific name.</p></li><li><p><code>volumes</code>: - <code>open-webui:/app/backend/data</code>: Similar to Ollama, this creates persistent storage for Open WebUI's data. /app/backend/data is where Open WebUI stores its data. open-webui: is another named volume. This keeps your Open WebUI settings and data safe.</p></li><li><p><code>depends_on</code>: - ollama: This is crucial. It tells Docker Compose that the Open WebUI service depends on the Ollama service. Docker Compose will start Ollama before starting Open WebUI. This is essential because Open WebUI needs Ollama to run and function. It's like saying, &quot;Don't start the dashboard until the engine runs”. Read more about <a href="/blog/2024/02/docker-compose-depends-on/">Docker compose depends on</a>.</p></li><li><p><code>ports</code>: <code>- 3000:8080</code> This maps port 3000 on your host machine to port 8080 inside the Open WebUI container. Open WebUI runs on port 8080. This means you'll access the Open WebUI interface by going to <a href="http://localhost:3000">http://localhost:3000</a> in your web browser.</p></li><li><p><code>environment</code>: This sets environment variables inside the Open WebUI container. These are configuration settings.</p></li><li><p><code>OLLAMA_BASE_URL=http://ollama:11434</code>: This tells Open WebUI where to find Ollama. Notice it's using the service name ollama (not localhost). Docker Compose sets up internal networking so services can communicate using their service names. This is how Open WebUI knows how to connect to the Ollama &quot;engine.&quot;</p></li><li><p><code>WEBUI_SECRET_KEY=</code>: This is a security setting for Open WebUI. You should set it to a strong, random value for production use. It's like a password for the web interface. Leaving it blank is fine for local testing but not for a public-facing server.</p></li><li><p><code>extra_hosts</code>: as <code>- host.docker.internal:host-gateway:</code> This is a bit more advanced. It allows the container to access services running on your host machine. <code>host.docker.internal</code> is a special hostname that resolves to your host's internal IP address. This is useful if, for example, you have another service running directly on your computer (not inside a container) that Open WebUI needs to access.</p></li><li><p><code>restart: unless-stopped</code>: As with Ollama, this ensures that Open WebUI restarts automatically unless you manually stop it.</p></li><li><p><code>volumes</code>: This section defines the named volumes used above. Volumes persist data even if the containers are restarted.</p></li><li><p><code>ollama: {}</code> defines the ollama volume. The empty {} means we're using the default Docker volume driver.</p></li><li><p><code>open-webui: {}</code> defines the open-webui volume using the default driver where Docker manages where and how to save it.</p></li></ul><p>This docker-compose file sets up a system with Ollama (likely a large language model server) and Open-WebUI (a web interface to interact with Ollama). It ensures that Ollama starts first, that both services have persistent storage, and that Open-WebUI knows how to connect to Ollama. You'll be able to access Open-WebUI on your computer at port 3000. Remember to set a <code>WEBUI_SECRET_KEY</code>!</p><h2 id="running-ollama-and-open-webui-with-docker-compose" tabindex="-1">Running Ollama and Open WebUI with Docker compose <a class="direct-link" href="#running-ollama-and-open-webui-with-docker-compose">#</a></h2><p>To run the above Docker Compose file, please execute:</p><pre class="language-bash"><code class="language-bash"><span class="token function">docker</span> compose up</code></pre><p>Or you could run <code>docker-compose up</code> depending on the version of Docker Compose installed on your machine. Running this command for the first time will take some time, depending on your internet speed, because it will download around 4 GB of data in total (2.5 GB for Ollama and 1.5 GB or a bit more for Open WebUI). So you can make your coffee now and come back with it when the download finishes:</p><img class="center" src="/images/ollama-docker-compose/02ollama-open-webui-pull-docker-images.jpg" loading="lazy" title="Pulling GBs or data for Ollama and Open WebUI Docker Images" alt="Pulling GBs or data for Ollama and Open WebUI Docker Images"><p>After it downloads both the docker images and runs them, you will see something like the below on the CLI:</p><img class="center" src="/images/ollama-docker-compose/03ollama-open-webui-running-with-docker-compose.jpg" loading="lazy" title="Ollama and Open WebUI running inside Docker with Docker Compose up" alt="Ollama and Open WebUI running inside Docker with Docker Compose up"><p>Now you can go to <code>http://localhost:3000</code> on the browser of your choice (probably Google Chrome), and you will see the following welcome screen of Open WebUI:</p><img class="center" src="/images/ollama-docker-compose/04open-webui-welcome.jpg" loading="lazy" title="First welcome screen of Open WebUI running inside Docker with Docker Compose" alt="First welcome screen of Open WebUI running inside Docker with Docker Compose"><p>Click the <code>Get Started</code> link, and then you will need to fill out the form as shown below:</p><img class="center" src="/images/ollama-docker-compose/05open-webui-admin-registration-form.jpg" loading="lazy" title="Open WebUI Registration form to register the admin user" alt="Open WebUI Registration form to register the admin user"><p>After you fill out the form, you will reach the Open WebUI Dashboard with an announcement:</p><img class="center" src="/images/ollama-docker-compose/06open-webui-registered.jpg" loading="lazy" title="Admin registered on Open WebUI and landing on the logged in page for the first time" alt="Admin registered on Open WebUI and landing on the logged in page for the first time"><p>Click on <code>Ok let’s go</code> to see the Open WebUI main screen. As no models are downloaded, you will download the <code>smollm2:135m</code> model using the UI. This can also be done from the CLI with <code>docker compose exec ollama ollama pull smollm2:135m</code>, but you will use the UI for now.</p><p>To pull/download the model onto your local Ollama instance, click the <code>Select a model</code> drop down and type in <code>smollm2:135m</code> then click on <code>Pull smollm2:135m from Ollama.com</code> to download the model as shown below:</p><img class="center" src="/images/ollama-docker-compose/07open-webui-download-smollm2.jpg" loading="lazy" title="Download the Smollm2:135m model from the Open WebUI interface" alt="Download the Smollm2:135m model from the Open WebUI interface"><p>It is a relatively small model at 271 MB, so depending on your internet speed, it will finish in seconds or a couple of minutes as follows:</p><img class="center" src="/images/ollama-docker-compose/08open-webui-downloading-smollm2.jpg" loading="lazy" title="Downloading the Smollm2:135m model from the Open WebUI interface" alt="Downloading the Smollm2:135m model from the Open WebUI interface"><p>After the model is downloaded locally on your machine and in the Ollama instance, you can start chatting or prompting the model. You can ask questions like <code>who are you?</code> or <code>why is the sky blue? give the shortest possible answer in under 20 words</code> as seen below:</p><img class="center" src="/images/ollama-docker-compose/09open-webui-smollm2-chat.jpg" loading="lazy" title="Chatting with Smollm2 135 million params on Open WebUI" alt="Chatting with Smollm2 135 million params on Open WebUI"><p>The model will reply. You can also configure the models by clicking the <code>settings</code> icon at the top right of the screen. Parameters like temperature, top K, Top P, and others can be changed on the Open WebUI configs as follows:</p><img class="center" src="/images/ollama-docker-compose/10open-webui-ollama-configs.jpg" loading="lazy" title="You can change Ollama configs on Open WebUI interface" alt="You can change Ollama configs on Open WebUI interface"><p>You now have your mini chatGPT running locally. Since it is Ollama and the model has been downloaded, it can run even without the internet on a plane. Depending on the resources available, such as disk space, CPU/GPU, and memory, you can download other models, such as Llama, Microsoft Phi, Gemma 2, or DeepSeek, from Ollama’s <a href="https://ollama.com/search">model registry</a>.</p><h2 id="the-docker-images-are-huge" tabindex="-1">The docker images are huge <a class="direct-link" href="#the-docker-images-are-huge">#</a></h2><p>The uncompressed Docker image for Ollama is 4.5 GB, which will grow bigger when you download a model. Similarly, the uncompressed image for Open WebUI is 3.77 GB. Both of them are huge, as you can see below:</p><img class="center" src="/images/ollama-docker-compose/11ollama-open-webui-docker-images-size.jpg" loading="lazy" title="Ollama and Open Web UI have big Docker images" alt="Ollama and Open Web UI have big Docker images"><p>Make sure to have at least 9-10 GB of free space on your hard disk before downloading these large Docker images.</p><h2 id="hosting-ollama-on-the-cloud" tabindex="-1">Hosting Ollama on the cloud <a class="direct-link" href="#hosting-ollama-on-the-cloud">#</a></h2><p>You can follow this step-by-step tutorial to run <a href="/blog/2025/01/ollama-google-cloud-run/">Ollama on Google Cloud Run</a>. If you are looking for a more production-ready Ollama docker image with a model (Gemma 2:9b) already pulled, have a look at this <a href="https://github.com/geshan/ollama-cloud-run/blob/master/Dockerfile">Dockerfile</a>. You can easily change the version of Ollama and also download another model of your choice to host it on Google Cloud Run in <a href="/blog/2023/04/serverless-containers/">serverless containers</a>. You can follow this <a href="https://codelabs.developers.google.com/codelabs/how-to-use-ollama-sidecar-open-webui-frontend-cloud-run-gpu#0">codelab</a> to create a multi-container Cloud Run service with Ollama and Open WebUI together on Google Cloud Run where Open WebUI is the main pod (ingress frontend) and Ollama is a sidecar.</p><p>As it is a Docker container, you can also run it on your Kubernetes cluster on GKE.</p><h2 id="conclusion" tabindex="-1">Conclusion <a class="direct-link" href="#conclusion">#</a></h2><p>This blog post explains how to run Ollama and Open WebUI with Docker Compose. Ollama is an open-source tool for running large language models (LLMs) on your machine, and Open WebUI provides a web-based chat interface for interacting with the models.</p><p>The blog post first recaps the Ollama blog series and lists the prerequisites for running Ollama and Open WebUI with Docker Compose. It then explains Open WebUI and provides the Ollama Docker Compose file. Next, it explains the Ollama Open WebUI Docker services and how to run them with Docker Compose. It also notes that the Docker images are large, and the post provides guidance for hosting Ollama on the cloud. Keep exploring!</p></div><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2025-02-11" data-updated="true"><time class="entry-date" datetime="2025-02-11"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">11-Feb-2025 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">11 MIN READ</span></div><button type="button" id="share-button-bottom" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIconsBottom(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container-bottom" class="share-icons-bottom print:hidden hidden mx-auto" aria-label="Share this post"><div class="w-full flex items-center justify-center mx-auto gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=How to use Ollama and Open WebUI with Docker Compose [Part 4] - &url=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2025/02/ollama-docker-compose/" target="_blank" title="Share 'How to use Ollama and Open WebUI with Docker Compose [Part 4]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></article><div class="mt-3"><a href="/blog/categories/ai/" title="AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">ai </a><a href="/blog/categories/gen-ai/" title="Gen AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">gen ai </a><a href="/blog/categories/ollama/" title="Ollama" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">ollama</a></div><section class="md:w-full mx-auto mt-4"><h3 class="comments font-heading">Comments</h3><div id="disqus_thread"></div><button id="disqus_trigger" class="font-ui text-blackText hover:border-darkAvocado border-blacktext hover:shadow-lg border-2 transition ease-in-out delay-100 px-3 py-2 pt-2 mt-2 rounded-md text-center block h-auto text-xl" onclick="load_disqus()">Post a Comment</button><div id="disqus_thread" aria-live="polite"></div></section></div></div><section class="md:w-full mx-auto mt-4 bg-lightAvocado p-4 rounded-lg pb-8"><div class="max-w-6xl mx-auto md:py-12 px-4"><h2 class="text-4xl text-darkAvocado font-heading font-regular flex-shrink-0 py-8 tracking-normal leading-[105%] md:leading-[105%]"><span class="text-lightGray italic">Related</span> <span class="text-blackText">Blogs</span></h2><div class="grid grid-cols-1 md:grid-cols-2 gap-8"><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/ollama-api/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">Using Ollama APIs to generate responses and much more [Part 3]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">09-Feb-2025 &nbsp;&nbsp;&nbsp; 16 min read</time><p class="text-xl text-gray font-ui">Learn how to use Ollama APIs like generate, chat and more like list model, pull model, etc with cURL and Jq with useful examples</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/ollama-commands/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">Ollama commands: How to use Ollama in the command line [Part 2]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">06-Feb-2025 &nbsp;&nbsp;&nbsp; 13 min read</time><p class="text-xl text-gray font-ui">Learn about the important Ollama commands to run Ollama on your local machine with Smollm2 and Qwen 2.5 models</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/what-is-ollama/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">What is Ollama and how to use it: a quick guide [part 1]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">02-Feb-2025 &nbsp;&nbsp;&nbsp; 9 min read</time><p class="text-xl text-gray font-ui">Learn what Ollama is, its features and how to run it on your local machine with DeepSeek R1 and Smollm2 models</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/01/ollama-google-cloud-run/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">20-Jan-2025 &nbsp;&nbsp;&nbsp; 11 min read</time><p class="text-xl text-gray font-ui">Learn how to run and host Gemma 2:2b with Ollama on Google Cloud Run in this step-by-step tutorial. You can use Gemma with an API, too, using Ollama</p></div></div></div></section><section class="bg-avocado md:py-0 py-12 px-4"><div class="max-w-6xl mx-auto"><div class="md:hidden"><h2 class="mb-4 text-3xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-base text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-base text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full text-center">Follow on LinkedIn &nbsp;→</a></div><div class="hidden md:grid lg:hidden grid-cols-3 gap-8 items-center"><div class="z-10 col-span-2"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity">Follow on LinkedIn &nbsp;→</a></div><div class="flex items-center justify-end col-span-1"><img class="w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"></div></div><div class="hidden lg:grid grid-cols-2 gap-8 lg:gap-12 items-center justify-between h-full"><div class="z-10"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><p class="text-xl text-blackText font-body font-regular mb-6">A big thank you for supporting this ad. free, unobstructive (no overlays, no pop-ups) blog.</p></div><div class="flex items-center gap-12 w-full md:w-auto justify-end self-end"><img class="w-16 md:w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"> <a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full md:w-auto text-center md:text-left">Follow on LinkedIn &nbsp;→</a></div></div></div></section></div></div></div></main><footer role="contentinfo"><footer class="bg-avocado sm:px-4"><div class="max-w-6xl mx-auto flex flex-col md:flex-row sm:flex-col justify-between gap-2 items-center sm:items-center md:items-center py-10 font-nav text-lightGray text-base font-regular"><div class="pb-0 sm:pb-2 text-gray"><span class="pr-4">Copyright © 2026 Geshan Manandhar.</span></div><div class="pb-0 sm:pb-2"><ul class="flex"><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> LinkedIn</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.twitter.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Twitter</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://github.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Github</a></li></ul></div></div></footer></footer><script src="/js/all.min.js" defer="defer"></script></body></html>