<!doctype html><html class="no-js" lang="en"><head><meta charset="utf-8"><meta name="author" content="Geshan Manandhar"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="Learn how to use Ollama APIs like generate, chat and more like list model, pull model, etc with cURL and Jq with useful examples"><meta name="keywords" content="ollama api, ollama apis, ollama api endpoints, ollama generate, ollama chat, ollama pull"><meta name="p:domain_verify" content="e654c68562abebfa25c291f59d7d00e8"><meta property="og:type" content="website"><meta property="og:url" content="https://geshan.com.np/blog/2025/02/ollama-api/"><meta property="og:title" content="Using Ollama APIs to generate responses and much more [Part 3]"><meta property="og:description" content="Learn how to use Ollama APIs like generate, chat and more like list model, pull model, etc with cURL and Jq with useful examples"><meta property="og:site_name" content="Geshan&#39;s Blog"><meta property="og:image" content="https://geshan.com.np/images/ollama-api/01ollama-api.jpg"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:widgets:new-embed-design" content="on"><meta name="twitter:site" content="@geshan"><meta name="twitter:creator" content="@geshan"><meta name="twitter:title" content="Using Ollama APIs to generate responses and much more [Part 3]"><meta name="twitter:description" content="Learn how to use Ollama APIs like generate, chat and more like list model, pull model, etc with cURL and Jq with useful examples"><meta name="twitter:image:src" content="https://geshan.com.np/images/ollama-api/01ollama-api.jpg"><link rel="canonical" href="https://geshan.com.np/blog/2025/02/ollama-api/"><meta property="fb:pages" content="30717799226"><meta property="fb:app_id" content="106030259434380"><meta name="monetization" content="$ilp.uphold.com/aKHWpqhphm9f"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicons/favicon-16x16.png"><link href="/atom.xml" rel="alternate" title="Geshan&#39;s Blog" type="application/atom+xml"><title>How to use Ollama APIs like generate, chat and more [with examples]</title><link rel="preconnect" href="/" crossorigin><link rel="preload" href="/css/fonts.css" as="style"><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-500-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-pro/source-sans-pro-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="/css/fonts.css"><link rel="stylesheet" href="/css/tw-006.css"><link rel="alternate" href="/atom.xml" type="application/atom+xml" title="Geshan&#39;s Blog"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-P3NXCVQEPE"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-P3NXCVQEPE');</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWT2D9T');</script><script src="https://cdn.jsdelivr.net/npm/@statsig/js-client@3/build/statsig-js-client+session-replay+web-analytics.min.js?apikey=client-rYO7rX8WSUyyLg9pKJwymLxM71tCE0CKgxgtUj4akzK"></script><link rel="manifest" href="/manifest.json"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="application-name" content="Geshan.com.np"><meta name="apple-mobile-web-app-title" content="Geshan.com.np"><meta name="msapplication-starturl" content="/index.html"><meta name="theme-color" content="#6947E7"><script>if ("serviceWorker" in navigator) {
    navigator.serviceWorker.register("/sw.js").then(function(registration) {
        console.log('ServiceWorker registration successful with scope: ', registration.scope);
    }, function(err) {
        console.log('ServiceWorker registration failed: ', err);
    });
  }</script></head><body class="overflow-x-hidden font-ui flex flex-col min-h-screen"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWT2D9T" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><header role="banner"><div><nav class="bg-white" role="navigation"><div class="w-full md:max-w-6xl mx-auto py-4 px-4"><div class="flex items-center justify-between h-16"><div class="flex-shrink-0"><a href="/" class="flex items-center"><img class="h-12 w-12" src="/images/theme/new_logo.svg" alt="Geshan G"></a></div><div class="flex items-center gap-12"><div class="hidden sm:flex items-center gap-12"><a href="/posts/1/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Posts </a><a href="/about/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Newsletter</a></div><form class="search flex items-center" action="https://www.google.com/search" method="GET"><input type="hidden" name="sitesearch" value="geshan.com.np"><div class="relative"><input class="bg-gray-100 border border-neutralGray rounded-lg py-2 pl-10 pr-4 font-body text-sm focus:outline-none focus:ring-2 focus:ring-avocado focus:border-transparent w-40" type="text" name="q" placeholder="Search" label="search"> <svg class="absolute left-3 top-1/2 transform -translate-y-1/2 w-4 h-4 text-neutralGray" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/></svg></div></form><button type="button" class="sm:hidden inline-flex items-center justify-center p-2 rounded-md text-black focus:outline-none focus:ring-2 focus:ring-inset focus:ring-avocado" onclick="mobileView()" aria-controls="mobile-menu" aria-expanded="false"><span class="sr-only">Open main menu</span> <svg class="w-6 h-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button></div></div></div><div class="mobile-menu fixed inset-0 z-40 hidden sm:hidden bg-black bg-opacity-40" role="dialog" aria-modal="true"><div class="absolute inset-0" onclick="mobileView()" aria-hidden="true"></div><div class="mobile-menu-panel relative ml-auto flex h-full w-10/12 max-w-sm flex-col justify-start bg-white shadow-xl"><div class="flex items-center justify-between px-4 py-4 border-b border-gray-200"><div class="flex items-center gap-3"><img class="h-8 w-8" src="/images/theme/new_logo.svg" alt="Geshan G"> <span class="text-lg font-semibold text-textColor font-body">Geshan's Blog</span></div><button type="button" class="inline-flex items-center justify-center rounded-full p-2 text-gray-700 hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-avocado" onclick="mobileView()" aria-label="Close main menu"><svg class="h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg></button></div><nav class="overflow-y-auto px-4 py-6 space-y-2"><a href="/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Home </a><a href="/posts/1/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Posts </a><a href="/about/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Newsletter</a></nav><div class="px 4 py-4 border-t border-gray-200"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="rounded-lg bg-darkAvocado px-4 py-2 text-base font-regular font-body text-white hover:bg-avocado hover:text-black transition-colors w-2/3 mx-auto ml-4">Connect on LinkedIn</a></div></div></div></nav></div></header><main id="wrap" role="main" class="flex-grow md:px-0"><div id="content"><div class="row"><div><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Geshan&#39;s Blog",
    "alternativeHeadline": "Using Ollama APIs to generate responses and much more [Part 3]",
    "image": "https://geshan.com.np/images/ollama-api/01ollama-api.jpg",
    "editor": "Geshan Manandhar",
    "genre": "AI",
    "keywords": "ollama api, ollama apis, ollama api endpoints, ollama generate, ollama chat, ollama pull",
    "url": "https://geshan.com.np/blog/2025/02/ollama-api/",
    "datePublished": "2025-02-09",
    "dateCreated": "2025-02-09",
    "dateModified": "2025-02-09",
    "description": "Learn how to use Ollama APIs like generate, chat and more like list model, pull model, etc with cURL and Jq with useful examples",
    "articleBody": "Ollama is open-source software that makes running most open LLMs seamlessly on your own machine (or even on the cloud). Written in Go lang, Ollama is user-friendly and easy to start. In this post, part 3 of the Ollama blog posts series, you will learn about using Ollama’s APIs for generating responses (LLM inference) and much more; let’s get going!Table of contents #Quick review of the Ollama seriesCurl and JqOllama API EndpointsGenerate endpointChat endpointList modelsPull a modelShow model informationOther Ollama APIsImportant caveatConclusionQuick review of the Ollama series #This blog post is part 3 of the Ollama series. In the first part, you learned about what Ollama is, including its features and also how to run Ollama on your local machine with a couple of models.In part two, you explored some useful Ollama commands, like ollama serve to start Ollama and serve available models, ollama run to pull (download) and run a model.In this part, you will learn about the Ollama APIs. In addition to the inference API endpoints  some code  forward to Ollama. As Ollama’s server is written in Go/Gin, you may even try that path to secure your APIs if you know how to write Go and Gin. There is an issue on the official Ollama GitHub repository about something similar if you want to follow that.Conclusion #In this post, you learned about some of the Ollama API endpoints, focusing on the ones that help you get a response from an open model. You learned about the Ollama API endpoints for pulling a model, listing models, and showing model information.More importantly, you are aware of a crucial caveat: you should not expose all the available Ollama APIs to the outside world. If someone calls the delete model API endpoint, your Ollama API will stop functioning, so be careful. Keep learning!",
    "author": { "@type": "Person", "name": "Geshan Manandhar" },
    "publisher": {
      "@type": "Organization",
      "name": "Geshan Manandhar",
      "logo": { "@type": "ImageObject", "url": "https://geshan.com.np/images/favicons/favicon-32x32.png" }
    },
    "mainEntityOfPage": { "@type": "WebPage", "@id": "https://geshan.com.np/blog/2025/02/ollama-api/" }
  }</script><div class="progress-bar"></div><div class="max-w-6xl py-3 mx-auto grid-cols-12 grid gap-15 px-4 md:px-0 mb-20"><div class="col-span-12"><article class="md:w-full mx-auto"><header class="page-header text-center"><h1 class="font-semibold font-heading text-gray text-center mb-6 tracking-[-0.01em] leading-[105%] md:leading-[105%]">Using Ollama APIs to generate responses and much more [Part 3]</h1><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2025-02-09" data-updated="true"><time class="entry-date" datetime="2025-02-09"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">09-Feb-2025 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">16 MIN READ</span></div><button type="button" id="share-button" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIcons(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container" class="share-icons print:hidden hidden" aria-label="Share this post"><div class="flex items-center justify-center gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=Using Ollama APIs to generate responses and much more [Part 3] - &url=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></header><div class="entry-content clearfix font-body text-xl text-gray"><p>Ollama is open-source software that makes running most open LLMs seamlessly on your own machine (or even on the cloud). Written in Go lang, Ollama is user-friendly and easy to start. In this post, part 3 of the Ollama blog posts series, you will learn about using Ollama’s APIs for generating responses (LLM inference) and much more; let’s get going!</p><img class="center" src="/images/ollama-api/01ollama-api.jpg" title="Using Ollama APIs to generate responses and much more [Part 3]" alt="Using Ollama APIs to generate responses and much more [Part 3]"><h2 id="table-of-contents" tabindex="-1">Table of contents <a class="direct-link" href="#table-of-contents">#</a></h2><ul><li><a href="#quick-review-of-the-ollama-series">Quick review of the Ollama series</a></li><li><a href="#curl-and-jq">Curl and Jq</a></li><li><a href="#ollama-api-endpoints">Ollama API Endpoints</a><ul><li><a href="#generate-endpoint">Generate endpoint</a></li><li><a href="#chat-endpoint">Chat endpoint</a></li><li><a href="#list-models">List models</a></li><li><a href="#pull-a-model">Pull a model</a></li><li><a href="#show-model-information">Show model information</a></li><li><a href="#other-ollama-apis">Other Ollama APIs</a></li></ul></li><li><a href="#important-caveat">Important caveat</a></li><li><a href="#conclusion">Conclusion</a></li></ul><h2 id="quick-review-of-the-ollama-series" tabindex="-1">Quick review of the Ollama series <a class="direct-link" href="#quick-review-of-the-ollama-series">#</a></h2><p>This blog post is part 3 of the Ollama series. In the first part, you learned about <a href="/blog/2025/02/what-is-ollama/#what-is-ollama">what Ollama is</a>, including its <a href="/blog/2025/02/what-is-ollama/#ollama-features">features</a> and also <a href="/blog/2025/02/what-is-ollama/#how-to-run-ollama-locally">how to run Ollama</a> on your local machine with a couple of models.</p><p>In part two, you explored some useful <a href="/blog/2025/02/ollama-commands/">Ollama commands</a>, like <a href="/blog/2025/02/ollama-commands/#ollama-serve">ollama serve</a> to start Ollama and serve available models, <a href="/blog/2025/02/ollama-commands/#ollama-run">ollama run</a> to pull (download) and run a model.</p><p>In this part, you will learn about the Ollama APIs. In addition to the inference API endpoints <code>/api/generate</code> and <code>/api/chat</code>, you will also learn about other useful API endpoints. The Ollama commands call these APIs behind the scenes to provide the outputs.</p><p>In the next part, part 4, you will learn how to run <a href="/blog/2025/02/ollama-docker-compose/">Ollama in Docker</a> with Docker Compose. You will also add Open WebUI with Docker Compose to have a WebUI to interact with LLMs running on Ollama.</p><h2 id="curl-and-jq" tabindex="-1">Curl and Jq <a class="direct-link" href="#curl-and-jq">#</a></h2><p>For this guide, you will use cURL to call the APIs with <a href="https://jqlang.org/">Jq</a>. To install JQ, follow their official <a href="https://jqlang.org/download/">download</a> guides for instance, on a Mac, you can run <code>brew install jq</code>, similarly on an Ubuntu machine, you can execute <code>sudo apt-get install jq</code> on a Windows machine, you can use chocolatey. Or you can get the binary and make it executable, too.</p><p>You can use other programming languages, such as Python and JavaScript, with the official libraries for <a href="https://github.com/ollama/ollama-python">Python</a> and <a href="https://github.com/ollama/ollama-js">JavaScript</a>, and frameworks like <a href="https://docs.litellm.ai/docs/providers/ollama">LiteLLM</a> or <a href="https://python.langchain.com/docs/integrations/llms/ollama/">LangChain</a> to call the Ollama APIs.</p><h2 id="ollama-api-endpoints" tabindex="-1">Ollama API Endpoints <a class="direct-link" href="#ollama-api-endpoints">#</a></h2><p>There are more than 10 Ollama API endpoints. This tutorial will focus on some of the most important ones. To use the APIs, you will need Ollama to run either with <code>ollama serve</code> or as a service. You will also need at least one model pulled for the API calls.</p><p>For this guide, you will use <a href="https://github.com/huggingface/smollm">smollm2</a>:135m, one of the smaller LLMs at 221 MB. You can use any bigger model if it runs on the available resources. The reason to choose <code>smollm2:135m</code> is because most machines, even with 512 MB or memory, can run it.</p><p>To start, run <code>ollama serve,</code> in another CLI tab, and run <code>ollama pull smollm2:135m</code>. If you have pulled <code>smollm2:135m</code> from the previous parts of this tutorial series, the download will be very fast as the files already exist.</p><h3 id="generate-endpoint" tabindex="-1">Generate endpoint <a class="direct-link" href="#generate-endpoint">#</a></h3><p>As the name points out, this API endpoint is available at <code>/api/generate</code> you can POST data to generate a response from the selected model for the provided prompt. It responds as a stream by default; you can configure it to return all the responses in one go and not as a stream. It has four parameters:</p><ul><li>model: the model's name, for example, <code>smollm2:135m</code>, is a required parameter.</li><li>prompt: is the prompt you want to send to the selected model like <code>Why is the sky blue?</code></li><li>suffix: can be used if you want to append some text after the response</li><li>images: a list of base64-encoded images if you want to use it with multimodal models like Llava or SmolVLM.</li></ul><p>There are other advanced parameters, too, like <code>stream,</code> which you can read about in the official <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion">docs</a>. For now, you can run the following curl command to get a response from <code>smollm2:!35m</code> model:</p><pre class="language-bash"><code class="language-bash"><span class="token function">curl</span> http://localhost:11434/api/generate <span class="token parameter variable">-d</span> <span class="token string">'{<br>  "model": "smollm2:135m",<br>  "prompt": "Why is the sky blue? Give the shortest answer possible in under 20 words",<br>  "stream": false<br>}'</span> <span class="token operator">|</span> jq <span class="token builtin class-name">.</span></code></pre><p>In the above <code>curl</code> command, you are using the <code>/api/generate</code> API endpoint and asking about the <code>smollm2:135m</code> model. <code>Why is the sky blue? Give the shortest answer possible in under 20 words</code>, and ask Ollama not to stream the output, so give the full answer in one go. Then, the output is piped to <code>jq</code>. This gives the following output:</p><pre class="language-json"><code class="language-json"><span class="token punctuation">{</span><br>  <span class="token property">"model"</span><span class="token operator">:</span> <span class="token string">"smollm2:135m"</span><span class="token punctuation">,</span><br>  <span class="token property">"created_at"</span><span class="token operator">:</span> <span class="token string">"2025-02-08T11:02:55.115275Z"</span><span class="token punctuation">,</span><br>  <span class="token property">"response"</span><span class="token operator">:</span> <span class="token string">"The sky's clear and deep color due to tiny oxygen molecules (O2) scattering the sun's light allowing us to perceive it as blue."</span><span class="token punctuation">,</span><br>  <span class="token property">"done"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span><br>  <span class="token property">"done_reason"</span><span class="token operator">:</span> <span class="token string">"stop"</span><span class="token punctuation">,</span><br>  <span class="token property">"context"</span><span class="token operator">:</span> <span class="token punctuation">[</span><br>    <span class="token comment">//a long array of numbers here</span><br>  <span class="token punctuation">]</span><span class="token punctuation">,</span><br>  <span class="token property">"total_duration"</span><span class="token operator">:</span> <span class="token number">302810709</span><span class="token punctuation">,</span><br>  <span class="token property">"load_duration"</span><span class="token operator">:</span> <span class="token number">13315375</span><span class="token punctuation">,</span><br>  <span class="token property">"prompt_eval_count"</span><span class="token operator">:</span> <span class="token number">47</span><span class="token punctuation">,</span><br>  <span class="token property">"prompt_eval_duration"</span><span class="token operator">:</span> <span class="token number">132000000</span><span class="token punctuation">,</span><br>  <span class="token property">"eval_count"</span><span class="token operator">:</span> <span class="token number">30</span><span class="token punctuation">,</span><br>  <span class="token property">"eval_duration"</span><span class="token operator">:</span> <span class="token number">156000000</span><br><span class="token punctuation">}</span></code></pre><p>If you were calling this API from another script or software system, you would be more concerned about the <code>response</code> column. If you just want to see the response from the model, you can use <code>jq</code> for that in the following way:</p><pre class="language-bash"><code class="language-bash"><span class="token function">curl</span> http://localhost:11434/api/generate <span class="token parameter variable">-d</span> <span class="token string">'{<br>  "model": "smollm2:135m",<br>  "prompt": "Why is the sky blue? Give the shortest answer possible in under 20 words",<br>  "stream": false<br>}'</span> <span class="token operator">|</span> jq <span class="token string">'.["response"]'</span></code></pre><p>You are asking <code>jq</code> to show only the <code>response</code> attribute from the JSON response rather than the whole response. It will look as follows:</p><img class="center" src="/images/ollama-api/02ollama-generate-api.jpg" loading="lazy" title="Output of calling the ollama generate API endpoint with cURL and jq" alt="Output of calling the ollama generate API endpoint with cURL and jq"><p>You can make many types of requests on the generated API endpoint. One useful one is <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#request-structured-outputs">structured output</a>, where you can specify the output structure as a JSON object. Using the seed option, you can have <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#request-reproducible-outputs">reproducible outputs</a>. If you provide an empty prompt, the model is loaded into memory. It would be advisable that you read the official <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion">docs</a> about the <code>generate</code> endpoint.</p><h3 id="chat-endpoint" tabindex="-1">Chat endpoint <a class="direct-link" href="#chat-endpoint">#</a></h3><p>The chat endpoint available at <code>/api/chat</code>, which also works with POST, is similar to the <code>generate</code> API. It generates the next message in a chat with a selected model. It is a streaming endpoint that will have a series of responses. You can turn off the streaming with the <code>”stream::false</code> parameter as seen below:</p><pre class="language-bash"><code class="language-bash"><span class="token function">curl</span> http://localhost:11434/api/chat <span class="token parameter variable">-d</span> <span class="token string">'{<br>  "model": "smollm2:135m",<br>  "stream":false,<br>  "messages": [<br>    {<br>      "role": "user",<br>      "content": "Why is the sky blue? Give the shortest answer possible in under 20 words"<br>    }<br>  ]<br>}'</span> <span class="token operator">|</span> jq <span class="token builtin class-name">.</span></code></pre><p>The output of the above cURL will be similar to the following:</p><pre class="language-json"><code class="language-json"><span class="token punctuation">{</span><br>  <span class="token property">"model"</span><span class="token operator">:</span> <span class="token string">"smollm2:135m"</span><span class="token punctuation">,</span><br>  <span class="token property">"created_at"</span><span class="token operator">:</span> <span class="token string">"2025-02-08T11:22:15.229839Z"</span><span class="token punctuation">,</span><br>  <span class="token property">"message"</span><span class="token operator">:</span> <span class="token punctuation">{</span><br>    <span class="token property">"role"</span><span class="token operator">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span><br>    <span class="token property">"content"</span><span class="token operator">:</span> <span class="token string">"The sky appears blue because when sunlight passes through Earth's atmosphere, it contains tiny molecules like water vapor and oxygen. These molecules scatter shorter wavelengths of light (such as blue and violet) more than the longer wavelengths (like red and orange), making the sky appear blue to our eyes. Thank you for pointing this out!"</span><br>  <span class="token punctuation">}</span><span class="token punctuation">,</span><br>  <span class="token property">"done_reason"</span><span class="token operator">:</span> <span class="token string">"stop"</span><span class="token punctuation">,</span><br>  <span class="token property">"done"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span><br>  <span class="token property">"total_duration"</span><span class="token operator">:</span> <span class="token number">407998750</span><span class="token punctuation">,</span><br>  <span class="token property">"load_duration"</span><span class="token operator">:</span> <span class="token number">6202542</span><span class="token punctuation">,</span><br>  <span class="token property">"prompt_eval_count"</span><span class="token operator">:</span> <span class="token number">47</span><span class="token punctuation">,</span><br>  <span class="token property">"prompt_eval_duration"</span><span class="token operator">:</span> <span class="token number">36000000</span><span class="token punctuation">,</span><br>  <span class="token property">"eval_count"</span><span class="token operator">:</span> <span class="token number">65</span><span class="token punctuation">,</span><br>  <span class="token property">"eval_duration"</span><span class="token operator">:</span> <span class="token number">363000000</span><br><span class="token punctuation">}</span></code></pre><p>As seen in the above response, the structure of the response is a bit different than the <code>generate</code> API endpoint. This one has a <code>message</code> attribute, which is an object, and a <code>content</code> attribute inside it. If you want to extract the content from the response using Jq, you can run the following command:</p><pre class="language-bash"><code class="language-bash"><span class="token function">curl</span> http://localhost:11434/api/chat <span class="token parameter variable">-d</span> <span class="token string">'{<br>  "model": "smollm2:135m",<br>  "stream":false,<br>  "messages": [<br>    {<br>      "role": "user",<br>      "content": "Why is the sky blue? Give the shortest answer possible in under 20 words"<br>    }<br>  ]<br>}'</span> <span class="token operator">|</span> jq <span class="token string">'.["message"]["content"]'</span></code></pre><p>The extracted content from the response of the <code>/api/chat</code> will look as follows:</p><img class="center" src="/images/ollama-api/03ollama-chat-api.jpg" loading="lazy" title="Output of calling the ollama chat API endpoint with cURL and jq" alt="Output of calling the ollama chat API endpoint with cURL and jq"><p>You can do <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#chat-request-structured-outputs">structured outputs</a> with the chat endpoint. Being a chat endpoint, you can send in the <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#chat-request-with-history">history</a> of the conversation to the the endpoint. For all the other types of reqeust, you can send to this <code>/api/chat</code> endpoint, it would be best to go through the <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion">official Ollama docs</a> about it.</p><h3 id="list-models" tabindex="-1">List models <a class="direct-link" href="#list-models">#</a></h3><p>To list the local models available, you can call the <code>/api/tags</code> endpoint, which works with a GET request. This endpoint lists models available locally that you can send as the model name parameter in the generate and chat endpoints. Below is an example call of this API:</p><pre class="language-bash"><code class="language-bash"><span class="token function">curl</span> http://localhost:11434/api/tags <span class="token operator">|</span> jq <span class="token builtin class-name">.</span></code></pre><p>It will output a JSON similar to the one seen below:</p><pre class="language-json"><code class="language-json"><span class="token punctuation">{</span><br>  <span class="token property">"models"</span><span class="token operator">:</span> <span class="token punctuation">[</span><br>    <span class="token punctuation">{</span><br>      <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"smollm2:135m"</span><span class="token punctuation">,</span><br>      <span class="token property">"model"</span><span class="token operator">:</span> <span class="token string">"smollm2:135m"</span><span class="token punctuation">,</span><br>      <span class="token property">"modified_at"</span><span class="token operator">:</span> <span class="token string">"2025-02-08T15:33:44.760304367+11:00"</span><span class="token punctuation">,</span><br>      <span class="token property">"size"</span><span class="token operator">:</span> <span class="token number">270898672</span><span class="token punctuation">,</span><br>      <span class="token property">"digest"</span><span class="token operator">:</span> <span class="token string">"9077fe9d2ae1a4a41a868836b56b8163731a8fe16621397028c2c76f838c6907"</span><span class="token punctuation">,</span><br>      <span class="token property">"details"</span><span class="token operator">:</span> <span class="token punctuation">{</span><br>        <span class="token property">"parent_model"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><br>        <span class="token property">"format"</span><span class="token operator">:</span> <span class="token string">"gguf"</span><span class="token punctuation">,</span><br>        <span class="token property">"family"</span><span class="token operator">:</span> <span class="token string">"llama"</span><span class="token punctuation">,</span><br>        <span class="token property">"families"</span><span class="token operator">:</span> <span class="token punctuation">[</span><br>          <span class="token string">"llama"</span><br>        <span class="token punctuation">]</span><span class="token punctuation">,</span><br>        <span class="token property">"parameter_size"</span><span class="token operator">:</span> <span class="token string">"134.52M"</span><span class="token punctuation">,</span><br>        <span class="token property">"quantization_level"</span><span class="token operator">:</span> <span class="token string">"F16"</span><br>      <span class="token punctuation">}</span><br>    <span class="token punctuation">}</span><span class="token punctuation">,</span>    <br>    <span class="token punctuation">{</span><br>      <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"qwen2.5:0.5b"</span><span class="token punctuation">,</span><br>      <span class="token property">"model"</span><span class="token operator">:</span> <span class="token string">"qwen2.5:0.5b"</span><span class="token punctuation">,</span><br>      <span class="token property">"modified_at"</span><span class="token operator">:</span> <span class="token string">"2025-02-06T15:59:09.320362549+11:00"</span><span class="token punctuation">,</span><br>      <span class="token property">"size"</span><span class="token operator">:</span> <span class="token number">397821319</span><span class="token punctuation">,</span><br>      <span class="token property">"digest"</span><span class="token operator">:</span> <span class="token string">"a8b0c51577010a279d933d14c2a8ab4b268079d44c5c8830c0a93900f1827c67"</span><span class="token punctuation">,</span><br>      <span class="token property">"details"</span><span class="token operator">:</span> <span class="token punctuation">{</span><br>        <span class="token property">"parent_model"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><br>        <span class="token property">"format"</span><span class="token operator">:</span> <span class="token string">"gguf"</span><span class="token punctuation">,</span><br>        <span class="token property">"family"</span><span class="token operator">:</span> <span class="token string">"qwen2"</span><span class="token punctuation">,</span><br>        <span class="token property">"families"</span><span class="token operator">:</span> <span class="token punctuation">[</span><br>          <span class="token string">"qwen2"</span><br>        <span class="token punctuation">]</span><span class="token punctuation">,</span><br>        <span class="token property">"parameter_size"</span><span class="token operator">:</span> <span class="token string">"494.03M"</span><span class="token punctuation">,</span><br>        <span class="token property">"quantization_level"</span><span class="token operator">:</span> <span class="token string">"Q4_K_M"</span><br>      <span class="token punctuation">}</span><br>    <span class="token punctuation">}</span><span class="token punctuation">,</span><br>    <span class="token punctuation">{</span><br>      <span class="token property">"name"</span><span class="token operator">:</span> <span class="token string">"deepseek-r1:8b"</span><span class="token punctuation">,</span><br>      <span class="token property">"model"</span><span class="token operator">:</span> <span class="token string">"deepseek-r1:8b"</span><span class="token punctuation">,</span><br>      <span class="token property">"modified_at"</span><span class="token operator">:</span> <span class="token string">"2025-02-02T13:33:29.069046707+11:00"</span><span class="token punctuation">,</span><br>      <span class="token property">"size"</span><span class="token operator">:</span> <span class="token number">4920738407</span><span class="token punctuation">,</span><br>      <span class="token property">"digest"</span><span class="token operator">:</span> <span class="token string">"28f8fd6cdc677661426adab9338ce3c013d7e69a5bea9e704b364171a5d61a10"</span><span class="token punctuation">,</span><br>      <span class="token property">"details"</span><span class="token operator">:</span> <span class="token punctuation">{</span><br>        <span class="token property">"parent_model"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span><br>        <span class="token property">"format"</span><span class="token operator">:</span> <span class="token string">"gguf"</span><span class="token punctuation">,</span><br>        <span class="token property">"family"</span><span class="token operator">:</span> <span class="token string">"llama"</span><span class="token punctuation">,</span><br>        <span class="token property">"families"</span><span class="token operator">:</span> <span class="token punctuation">[</span><br>          <span class="token string">"llama"</span><br>        <span class="token punctuation">]</span><span class="token punctuation">,</span><br>        <span class="token property">"parameter_size"</span><span class="token operator">:</span> <span class="token string">"8.0B"</span><span class="token punctuation">,</span><br>        <span class="token property">"quantization_level"</span><span class="token operator">:</span> <span class="token string">"Q4_K_M"</span><br>      <span class="token punctuation">}</span><br>    <span class="token punctuation">}</span><br>  <span class="token punctuation">]</span><br><span class="token punctuation">}</span></code></pre><p>I have these three models available on my machine. Your response might be slightly different, but the output structure will remain the same.</p><h3 id="pull-a-model" tabindex="-1">Pull a model <a class="direct-link" href="#pull-a-model">#</a></h3><p>To download a new model from Ollama <a href="https://ollama.com/search">model registry</a> you can use the <code>/api/pull</code> API endpoint that works with a POST call. As the official doc states, canceled pulls are resumed, and in case of multiple pull calls they will share the same download progress.</p><p>A model name is required to pull a model, and you can choose to stream or not stream the response. Below is an example of calling the <code>/api/pull</code> endpoint without streaming to pull/download the <code>snowflake-arctic-embed:22m</code>, which is an embedding model at 46 MB:</p><pre class="language-bash"><code class="language-bash"><span class="token function">curl</span> http://localhost:11434/api/pull <span class="token parameter variable">-d</span> <span class="token string">'{<br>  "model": "snowflake-arctic-embed:22m"<br>}'</span> <span class="token operator">|</span> jq <span class="token builtin class-name">.</span></code></pre><p>It gives out a very long output with all the information about the pull (download) and ends with:</p><pre class="language-json"><code class="language-json"><span class="token punctuation">{</span><br>  <span class="token property">"status"</span><span class="token operator">:</span> <span class="token string">"pulling 2977e9705f4b"</span><span class="token punctuation">,</span><br>  <span class="token property">"digest"</span><span class="token operator">:</span> <span class="token string">"sha256:2977e9705f4b122813b1aeb50fc0c6563113da0b626f540c3daa8149827e30d3"</span><span class="token punctuation">,</span><br>  <span class="token property">"total"</span><span class="token operator">:</span> <span class="token number">333</span><span class="token punctuation">,</span><br>  <span class="token property">"completed"</span><span class="token operator">:</span> <span class="token number">333</span><br><span class="token punctuation">}</span><br><span class="token punctuation">{</span><br>  <span class="token property">"status"</span><span class="token operator">:</span> <span class="token string">"verifying sha256 digest"</span><br><span class="token punctuation">}</span><br><span class="token punctuation">{</span><br>  <span class="token property">"status"</span><span class="token operator">:</span> <span class="token string">"writing manifest"</span><br><span class="token punctuation">}</span><br><span class="token punctuation">{</span><br>  <span class="token property">"status"</span><span class="token operator">:</span> <span class="token string">"success"</span><br><span class="token punctuation">}</span></code></pre><p>Given that the model is downloaded, the easiest way to verify this is by running the <code>ollama list</code> command, and you will see the <code>snowflake-arctic-embed:22m</code> on the list, a 45 MB model. Using this model, you can use the <code>/api/embed</code> endpoint to generate the embeddings.</p><h3 id="show-model-information" tabindex="-1">Show model information <a class="direct-link" href="#show-model-information">#</a></h3><p>By calling the '/api/show ' endpoint with a POST call, you can view the model information, including details, model file, template, parameters, license, and system prompt. The model name is a required parameter. Passing the <code>verbose</code> optional parameter will return the full data with verbose fields in the response.</p><p>Below is an example call to the show model information endpoint without the verbose flag for the <code>smollm2:135m</code> model, some fields have been truncated for brevity:</p><pre class="language-json"><code class="language-json"><span class="token punctuation">{</span><br>  <span class="token property">"license"</span><span class="token operator">:</span> <span class="token string">"\nApache License\n Version 2.0, January 2004...the full apache license here"</span><span class="token punctuation">,</span><br>  <span class="token property">"modelfile"</span><span class="token operator">:</span> <span class="token string">"# Modelfile generated by \"ollama show\"\n# To build a new Modelfile based on this, replace FROM with:\n# FROM smollm2:135m\n\nFROM /path/to/.ollama/models/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57\nTEMPLATE \"\"\"template and parameter info here\nLICENSE \"\"\"\nApache License\n    Version 2.0, January 2004...the full apache license here"</span><span class="token punctuation">,</span><br>  <span class="token property">"parameters"</span><span class="token operator">:</span> <span class="token string">"stop\"&lt;|im_start|>\"\nstop\"&lt;|im_end|>\""</span><span class="token punctuation">,</span><br>  <span class="token property">"template"</span><span class="token operator">:</span> <span class="token string">"template info here"</span><span class="token punctuation">,</span><br>  <span class="token property">"system"</span><span class="token operator">:</span> <span class="token string">"You are a helpful AI assistant named SmolLM, trained by Hugging Face"</span><span class="token punctuation">,</span><br>  <span class="token property">"details"</span><span class="token operator">:</span> <span class="token punctuation">{</span><br>    <span class="token property">"parent_model"</span><span class="token operator">:</span> <span class="token string">"/path/to/.ollama/models/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57"</span><span class="token punctuation">,</span><br>    <span class="token property">"format"</span><span class="token operator">:</span> <span class="token string">"gguf"</span><span class="token punctuation">,</span><br>    <span class="token property">"family"</span><span class="token operator">:</span> <span class="token string">"llama"</span><span class="token punctuation">,</span><br>    <span class="token property">"families"</span><span class="token operator">:</span> <span class="token punctuation">[</span><br>      <span class="token string">"llama"</span><br>    <span class="token punctuation">]</span><span class="token punctuation">,</span><br>    <span class="token property">"parameter_size"</span><span class="token operator">:</span> <span class="token string">"134.52M"</span><span class="token punctuation">,</span><br>    <span class="token property">"quantization_level"</span><span class="token operator">:</span> <span class="token string">"F16"</span><br>  <span class="token punctuation">}</span><span class="token punctuation">,</span><br>  <span class="token property">"model_info"</span><span class="token operator">:</span> <span class="token punctuation">{</span><br>    <span class="token property">"general.architecture"</span><span class="token operator">:</span> <span class="token string">"llama"</span><span class="token punctuation">,</span><br>    <span class="token property">"general.basename"</span><span class="token operator">:</span> <span class="token string">"smollm2"</span><span class="token punctuation">,</span><br>    <span class="token property">"general.file_type"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span><br>    <span class="token property">"general.finetune"</span><span class="token operator">:</span> <span class="token string">"8k-lc100k-mix1-ep2"</span><span class="token punctuation">,</span><br>    <span class="token property">"general.languages"</span><span class="token operator">:</span> <span class="token punctuation">[</span><br>      <span class="token string">"en"</span><br>    <span class="token punctuation">]</span><span class="token punctuation">,</span><br>    <span class="token property">"general.license"</span><span class="token operator">:</span> <span class="token string">"apache-2.0"</span><span class="token punctuation">,</span><br>    <span class="token property">"general.organization"</span><span class="token operator">:</span> <span class="token string">"HuggingFaceTB"</span><span class="token punctuation">,</span><br>    <span class="token property">"general.parameter_count"</span><span class="token operator">:</span> <span class="token number">134515008</span><span class="token punctuation">,</span><br>    <span class="token property">"general.quantization_version"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span><br>    <span class="token property">"general.size_label"</span><span class="token operator">:</span> <span class="token string">"135M"</span><span class="token punctuation">,</span><br>    <span class="token property">"general.type"</span><span class="token operator">:</span> <span class="token string">"model"</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.attention.head_count"</span><span class="token operator">:</span> <span class="token number">9</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.attention.head_count_kv"</span><span class="token operator">:</span> <span class="token number">3</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.attention.layer_norm_rms_epsilon"</span><span class="token operator">:</span> <span class="token number">0.00001</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.block_count"</span><span class="token operator">:</span> <span class="token number">30</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.context_length"</span><span class="token operator">:</span> <span class="token number">8192</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.embedding_length"</span><span class="token operator">:</span> <span class="token number">576</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.feed_forward_length"</span><span class="token operator">:</span> <span class="token number">1536</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.rope.dimension_count"</span><span class="token operator">:</span> <span class="token number">64</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.rope.freq_base"</span><span class="token operator">:</span> <span class="token number">100000</span><span class="token punctuation">,</span><br>    <span class="token property">"llama.vocab_size"</span><span class="token operator">:</span> <span class="token number">49152</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.add_bos_token"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.add_space_prefix"</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.bos_token_id"</span><span class="token operator">:</span> <span class="token number">1</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.eos_token_id"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.merges"</span><span class="token operator">:</span> <span class="token null keyword">null</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.model"</span><span class="token operator">:</span> <span class="token string">"gpt2"</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.padding_token_id"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.pre"</span><span class="token operator">:</span> <span class="token string">"smollm"</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.token_type"</span><span class="token operator">:</span> <span class="token null keyword">null</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.tokens"</span><span class="token operator">:</span> <span class="token null keyword">null</span><span class="token punctuation">,</span><br>    <span class="token property">"tokenizer.ggml.unknown_token_id"</span><span class="token operator">:</span> <span class="token number">0</span><br>  <span class="token punctuation">}</span><span class="token punctuation">,</span><br>  <span class="token property">"modified_at"</span><span class="token operator">:</span> <span class="token string">"2025-02-08T15:33:44.760304367+11:00"</span><br><span class="token punctuation">}</span></code></pre><p>Depending on the level of security needed for your Ollama instance, the show model API should not be accessible outside of the app.</p><h3 id="other-ollama-apis" tabindex="-1">Other Ollama APIs <a class="direct-link" href="#other-ollama-apis">#</a></h3><p>Other Ollama APIs can <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#list-running-models">list running models</a>, <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#delete-a-model">delete a model</a> (you would not want someone to delete a pulled model), <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-model">create a model</a> from another model, <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#copy-a-model">copy a model</a>, and even <a href="https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings">generate embeddings</a>. You can explore them all in the official <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">documents</a>.</p><p>Suppose you have used Postman and its collections. You can also use this <a href="https://www.postman.com/postman-student-programs/ollama-api/documentation/suc47x8/ollama-rest-api">Postman collection</a> that lists most of the Ollama API calls in a neat, easy to test collection. In the next part (part 4) of the Ollama blog series, you will learn how to run Ollama in Docker with Docker Compose.</p><h2 id="important-caveat" tabindex="-1">Important caveat <a class="direct-link" href="#important-caveat">#</a></h2><p>If you plan to host Ollma on a publicly accessible URL or with some form of authentication and authorization, please remember to expose only the generate (<code>/api/generate</code>) and the chat (<code>/api/chat</code>) endpoints. You will not want users to call the pull or even delete model API endpoints.</p><p>You can do it by putting a reverse proxy in front of Ollama’s Gin server (running at port 11434, by default). You can choose between <a href="https://github.com/kesor/ollama-proxy">Nginx reverse proxy</a> or <a href="https://caddyserver.com/docs/caddyfile/directives/reverse_proxy">Caddy</a>. With a reverse proxy, you can pass through only the traffic that comes to <code>/api/generate</code> and <code>/api/chat</code> forward to Ollama. As Ollama’s server is written in Go/Gin, you may even try that path to secure your APIs if you know how to write Go and Gin. There is an <a href="https://github.com/ollama/ollama/issues/1053">issue</a> on the official Ollama GitHub repository about something similar if you want to follow that.</p><h2 id="conclusion" tabindex="-1">Conclusion <a class="direct-link" href="#conclusion">#</a></h2><p>In this post, you learned about some of the Ollama API endpoints, focusing on the ones that help you get a response from an open model. You learned about the Ollama API endpoints for pulling a model, listing models, and showing model information.</p><p>More importantly, you are aware of a crucial caveat: you should not expose all the available Ollama APIs to the outside world. If someone calls the delete model API endpoint, your Ollama API will stop functioning, so be careful. Keep learning!</p></div><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2025-02-09" data-updated="true"><time class="entry-date" datetime="2025-02-09"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">09-Feb-2025 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">16 MIN READ</span></div><button type="button" id="share-button-bottom" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIconsBottom(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container-bottom" class="share-icons-bottom print:hidden hidden mx-auto" aria-label="Share this post"><div class="w-full flex items-center justify-center mx-auto gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=Using Ollama APIs to generate responses and much more [Part 3] - &url=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2025/02/ollama-api/" target="_blank" title="Share 'Using Ollama APIs to generate responses and much more [Part 3]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></article><div class="mt-3"><a href="/blog/categories/ai/" title="AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">ai </a><a href="/blog/categories/gen-ai/" title="Gen AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">gen ai </a><a href="/blog/categories/ollama/" title="Ollama" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">ollama</a></div><section class="md:w-full mx-auto mt-4"><h3 class="comments font-heading">Comments</h3><div id="disqus_thread"></div><button id="disqus_trigger" class="font-ui text-blackText hover:border-darkAvocado border-blacktext hover:shadow-lg border-2 transition ease-in-out delay-100 px-3 py-2 pt-2 mt-2 rounded-md text-center block h-auto text-xl" onclick="load_disqus()">Post a Comment</button><div id="disqus_thread" aria-live="polite"></div></section></div></div><section class="md:w-full mx-auto mt-4 bg-lightAvocado p-4 rounded-lg pb-8"><div class="max-w-6xl mx-auto md:py-12 px-4"><h2 class="text-4xl text-darkAvocado font-heading font-regular flex-shrink-0 py-8 tracking-normal leading-[105%] md:leading-[105%]"><span class="text-lightGray italic">Related</span> <span class="text-blackText">Blogs</span></h2><div class="grid grid-cols-1 md:grid-cols-2 gap-8"><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/ollama-docker-compose/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to use Ollama and Open WebUI with Docker Compose [Part 4]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">11-Feb-2025 &nbsp;&nbsp;&nbsp; 11 min read</time><p class="text-xl text-gray font-ui">Learn how to use Ollama and Open WebUI inside Docker with Docker compose to run any open LLM and create your own mini ChatGPT.</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/ollama-commands/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">Ollama commands: How to use Ollama in the command line [Part 2]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">06-Feb-2025 &nbsp;&nbsp;&nbsp; 13 min read</time><p class="text-xl text-gray font-ui">Learn about the important Ollama commands to run Ollama on your local machine with Smollm2 and Qwen 2.5 models</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/what-is-ollama/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">What is Ollama and how to use it: a quick guide [part 1]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">02-Feb-2025 &nbsp;&nbsp;&nbsp; 9 min read</time><p class="text-xl text-gray font-ui">Learn what Ollama is, its features and how to run it on your local machine with DeepSeek R1 and Smollm2 models</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/01/ollama-google-cloud-run/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">20-Jan-2025 &nbsp;&nbsp;&nbsp; 11 min read</time><p class="text-xl text-gray font-ui">Learn how to run and host Gemma 2:2b with Ollama on Google Cloud Run in this step-by-step tutorial. You can use Gemma with an API, too, using Ollama</p></div></div></div></section><section class="bg-avocado md:py-0 py-12 px-4"><div class="max-w-6xl mx-auto"><div class="md:hidden"><h2 class="mb-4 text-3xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-base text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-base text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full text-center">Follow on LinkedIn &nbsp;→</a></div><div class="hidden md:grid lg:hidden grid-cols-3 gap-8 items-center"><div class="z-10 col-span-2"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity">Follow on LinkedIn &nbsp;→</a></div><div class="flex items-center justify-end col-span-1"><img class="w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"></div></div><div class="hidden lg:grid grid-cols-2 gap-8 lg:gap-12 items-center justify-between h-full"><div class="z-10"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><p class="text-xl text-blackText font-body font-regular mb-6">A big thank you for supporting this ad. free, unobstructive (no overlays, no pop-ups) blog.</p></div><div class="flex items-center gap-12 w-full md:w-auto justify-end self-end"><img class="w-16 md:w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"> <a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full md:w-auto text-center md:text-left">Follow on LinkedIn &nbsp;→</a></div></div></div></section></div></div></div></main><footer role="contentinfo"><footer class="bg-avocado sm:px-4"><div class="max-w-6xl mx-auto flex flex-col md:flex-row sm:flex-col justify-between gap-2 items-center sm:items-center md:items-center py-10 font-nav text-lightGray text-base font-regular"><div class="pb-0 sm:pb-2 text-gray"><span class="pr-4">Copyright © 2026 Geshan Manandhar.</span></div><div class="pb-0 sm:pb-2"><ul class="flex"><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> LinkedIn</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.twitter.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Twitter</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://github.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Github</a></li></ul></div></div></footer></footer><script src="/js/all.min.js" defer="defer"></script></body></html>