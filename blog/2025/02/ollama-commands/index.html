<!doctype html><html class="no-js" lang="en"><head><meta charset="utf-8"><meta name="author" content="Geshan Manandhar"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="Learn about the important Ollama commands to run Ollama on your local machine with Smollm2 and Qwen 2.5 models"><meta name="keywords" content="ollama commands, ollama serve, ollama run, ollama list, ollama ps"><meta name="p:domain_verify" content="e654c68562abebfa25c291f59d7d00e8"><meta property="og:type" content="website"><meta property="og:url" content="https://geshan.com.np/blog/2025/02/ollama-commands/"><meta property="og:title" content="Ollama commands: How to use Ollama in the command line [Part 2]"><meta property="og:description" content="Learn about the important Ollama commands to run Ollama on your local machine with Smollm2 and Qwen 2.5 models"><meta property="og:site_name" content="Geshan&#39;s Blog"><meta property="og:image" content="https://geshan.com.np/images/ollama-commands/01ollama-commands.jpg"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:widgets:new-embed-design" content="on"><meta name="twitter:site" content="@geshan"><meta name="twitter:creator" content="@geshan"><meta name="twitter:title" content="Ollama commands: How to use Ollama in the command line [Part 2]"><meta name="twitter:description" content="Learn about the important Ollama commands to run Ollama on your local machine with Smollm2 and Qwen 2.5 models"><meta name="twitter:image:src" content="https://geshan.com.np/images/ollama-commands/01ollama-commands.jpg"><link rel="canonical" href="https://geshan.com.np/blog/2025/02/ollama-commands/"><meta property="fb:pages" content="30717799226"><meta property="fb:app_id" content="106030259434380"><meta name="monetization" content="$ilp.uphold.com/aKHWpqhphm9f"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicons/favicon-16x16.png"><link href="/atom.xml" rel="alternate" title="Geshan&#39;s Blog" type="application/atom+xml"><title>Ollama commands: How to use Ollama in the command line</title><link rel="preconnect" href="/" crossorigin><link rel="preload" href="/css/fonts.css" as="style"><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-500-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-pro/source-sans-pro-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="/css/fonts.css"><link rel="stylesheet" href="/css/tw-006.css"><link rel="alternate" href="/atom.xml" type="application/atom+xml" title="Geshan&#39;s Blog"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-P3NXCVQEPE"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-P3NXCVQEPE');</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWT2D9T');</script><script src="https://cdn.jsdelivr.net/npm/@statsig/js-client@3/build/statsig-js-client+session-replay+web-analytics.min.js?apikey=client-rYO7rX8WSUyyLg9pKJwymLxM71tCE0CKgxgtUj4akzK"></script><link rel="manifest" href="/manifest.json"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="application-name" content="Geshan.com.np"><meta name="apple-mobile-web-app-title" content="Geshan.com.np"><meta name="msapplication-starturl" content="/index.html"><meta name="theme-color" content="#6947E7"><script>if ("serviceWorker" in navigator) {
    navigator.serviceWorker.register("/sw.js").then(function(registration) {
        console.log('ServiceWorker registration successful with scope: ', registration.scope);
    }, function(err) {
        console.log('ServiceWorker registration failed: ', err);
    });
  }</script></head><body class="overflow-x-hidden font-ui flex flex-col min-h-screen"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWT2D9T" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><header role="banner"><div><nav class="bg-white" role="navigation"><div class="w-full md:max-w-6xl mx-auto py-4 px-4"><div class="flex items-center justify-between h-16"><div class="flex-shrink-0"><a href="/" class="flex items-center"><img class="h-12 w-12" src="/images/theme/new_logo.svg" alt="Geshan G"></a></div><div class="flex items-center gap-12"><div class="hidden sm:flex items-center gap-12"><a href="/posts/1/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Posts </a><a href="/about/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Newsletter</a></div><form class="search flex items-center" action="https://www.google.com/search" method="GET"><input type="hidden" name="sitesearch" value="geshan.com.np"><div class="relative"><input class="bg-gray-100 border border-neutralGray rounded-lg py-2 pl-10 pr-4 font-body text-sm focus:outline-none focus:ring-2 focus:ring-avocado focus:border-transparent w-40" type="text" name="q" placeholder="Search" label="search"> <svg class="absolute left-3 top-1/2 transform -translate-y-1/2 w-4 h-4 text-neutralGray" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/></svg></div></form><button type="button" class="sm:hidden inline-flex items-center justify-center p-2 rounded-md text-black focus:outline-none focus:ring-2 focus:ring-inset focus:ring-avocado" onclick="mobileView()" aria-controls="mobile-menu" aria-expanded="false"><span class="sr-only">Open main menu</span> <svg class="w-6 h-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button></div></div></div><div class="mobile-menu fixed inset-0 z-40 hidden sm:hidden bg-black bg-opacity-40" role="dialog" aria-modal="true"><div class="absolute inset-0" onclick="mobileView()" aria-hidden="true"></div><div class="mobile-menu-panel relative ml-auto flex h-full w-10/12 max-w-sm flex-col justify-start bg-white shadow-xl"><div class="flex items-center justify-between px-4 py-4 border-b border-gray-200"><div class="flex items-center gap-3"><img class="h-8 w-8" src="/images/theme/new_logo.svg" alt="Geshan G"> <span class="text-lg font-semibold text-textColor font-body">Geshan's Blog</span></div><button type="button" class="inline-flex items-center justify-center rounded-full p-2 text-gray-700 hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-avocado" onclick="mobileView()" aria-label="Close main menu"><svg class="h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg></button></div><nav class="overflow-y-auto px-4 py-6 space-y-2"><a href="/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Home </a><a href="/posts/1/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Posts </a><a href="/about/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Newsletter</a></nav><div class="px 4 py-4 border-t border-gray-200"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="rounded-lg bg-darkAvocado px-4 py-2 text-base font-regular font-body text-white hover:bg-avocado hover:text-black transition-colors w-2/3 mx-auto ml-4">Connect on LinkedIn</a></div></div></div></nav></div></header><main id="wrap" role="main" class="flex-grow md:px-0"><div id="content"><div class="row"><div><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Geshan&#39;s Blog",
    "alternativeHeadline": "Ollama commands: How to use Ollama in the command line [Part 2]",
    "image": "https://geshan.com.np/images/ollama-commands/01ollama-commands.jpg",
    "editor": "Geshan Manandhar",
    "genre": "AI",
    "keywords": "ollama commands, ollama serve, ollama run, ollama list, ollama ps",
    "url": "https://geshan.com.np/blog/2025/02/ollama-commands/",
    "datePublished": "2025-02-06",
    "dateCreated": "2025-02-06",
    "dateModified": "2025-02-06",
    "description": "Learn about the important Ollama commands to run Ollama on your local machine with Smollm2 and Qwen 2.5 models",
    "articleBody": "Ollama is an open-source tool that helps you run open LLMs on your machine or a server. It is the glue layer between your machine (or hardware) and the open LLM of your choice. In this post, you will learn about the Ollama command you can use to get the most out of it; let’s get going!Table of contents #Quick recapOllama commandsOllama serveOllama runOllama listOllama pullOllama psOllama createOther Ollama commandsConclusionQuick recap #This blog post is part 2 of the Ollama series. In the first part, you covered topics like what is Ollama, it’s features and how to run Ollama with examples of Smollm2 and DeepSeek R1 models.In this part, you will learn about some useful Ollama commands like serve, run, and ps. Before diving deeper into code mode, please ensure you have Ollama installed and working in your system by reading part 1. Part 1 also covers the installation of Ollama and running the Smollm2 135 million parameter model and DeepSeek R1 8 billion parameter model.The part 3 of this Ollama series covers the Ollama APIs, which are used by the CLI and can be used by other systems to interact with the LLMs.In part 4, you will learn about running Ollama in Docker with Docker Compose. You will also add Open WebUI in a Docker container to interact with the LLMs running on Ollama with Docker Compose.Ollama commands #Ollama has multiple commands to achieve relative goals. To know the sub-commands you can run with Ollama, you can execute the following: ollama --helpIt will give you the following output:You can also run  some code . By mastering these Ollama commands, you&#39;ll be well-equipped to harness the full potential of this powerful and easy-to-use framework, opening up a world of possibilities for your projects and applications. Whether you&#39;re a seasoned developer or just starting your journey into AI, Ollama and its commands will undoubtedly be invaluable assets in your toolkit. Keep learning!",
    "author": { "@type": "Person", "name": "Geshan Manandhar" },
    "publisher": {
      "@type": "Organization",
      "name": "Geshan Manandhar",
      "logo": { "@type": "ImageObject", "url": "https://geshan.com.np/images/favicons/favicon-32x32.png" }
    },
    "mainEntityOfPage": { "@type": "WebPage", "@id": "https://geshan.com.np/blog/2025/02/ollama-commands/" }
  }</script><div class="progress-bar"></div><div class="max-w-6xl py-3 mx-auto grid-cols-12 grid gap-15 px-4 md:px-0 mb-20"><div class="col-span-12"><article class="md:w-full mx-auto"><header class="page-header text-center"><h1 class="font-semibold font-heading text-gray text-center mb-6 tracking-[-0.01em] leading-[105%] md:leading-[105%]">Ollama commands: How to use Ollama in the command line [Part 2]</h1><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2025-02-06" data-updated="true"><time class="entry-date" datetime="2025-02-06"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">06-Feb-2025 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">13 MIN READ</span></div><button type="button" id="share-button" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIcons(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container" class="share-icons print:hidden hidden" aria-label="Share this post"><div class="flex items-center justify-center gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=Ollama commands: How to use Ollama in the command line [Part 2] - &url=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></header><div class="entry-content clearfix font-body text-xl text-gray"><p>Ollama is an open-source tool that helps you run open LLMs on your machine or a server. It is the glue layer between your machine (or hardware) and the open LLM of your choice. In this post, you will learn about the Ollama command you can use to get the most out of it; let’s get going!</p><img class="center" src="/images/ollama-commands/01ollama-commands.jpg" title="Ollama commands: How to use Ollama in the command line [Part 2]" alt="Ollama commands: How to use Ollama in the command line [Part 2]"><h2 id="table-of-contents" tabindex="-1">Table of contents <a class="direct-link" href="#table-of-contents">#</a></h2><ul><li><a href="#quick-recap">Quick recap</a></li><li><a href="#ollama-commands">Ollama commands</a><ul><li><a href="#ollama-serve">Ollama serve</a></li><li><a href="#ollama-run">Ollama run</a></li><li><a href="#ollama-list">Ollama list</a></li><li><a href="#ollama-pull">Ollama pull</a></li><li><a href="#ollama-ps">Ollama ps</a></li><li><a href="#ollama-create">Ollama create</a></li><li><a href="#other-ollama-commands">Other Ollama commands</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul><h2 id="quick-recap" tabindex="-1">Quick recap <a class="direct-link" href="#quick-recap">#</a></h2><p>This blog post is part 2 of the Ollama series. In the <a href="/blog/2025/02/what-is-ollama/">first part</a>, you covered topics like <a href="/blog/2025/02/what-is-ollama/#what-is-ollama">what is Ollama</a>, it’s <a href="/blog/2025/02/what-is-ollama/#ollama-features">features</a> and <a href="/blog/2025/02/what-is-ollama/#how-to-run-ollama-locally">how to run Ollama</a> with examples of Smollm2 and DeepSeek R1 models.</p><p>In this part, you will learn about some useful Ollama commands like serve, run, and ps. Before diving deeper into code mode, please ensure you have Ollama installed and working in your system by reading <a href="/blog/2025/02/what-is-ollama/">part 1</a>. Part 1 also covers the installation of Ollama and running the Smollm2 135 million parameter model and DeepSeek R1 8 billion parameter model.</p><p>The part 3 of this Ollama series covers the <a href="/blog/2025/02/ollama-api/">Ollama APIs</a>, which are used by the CLI and can be used by other systems to interact with the LLMs.</p><p>In part 4, you will learn about running <a href="/blog/2025/02/ollama-docker-compose/">Ollama in Docker with Docker Compose</a>. You will also add Open WebUI in a Docker container to interact with the LLMs running on Ollama with Docker Compose.</p><h2 id="ollama-commands" tabindex="-1">Ollama commands <a class="direct-link" href="#ollama-commands">#</a></h2><p>Ollama has multiple commands to achieve relative goals. To know the sub-commands you can run with Ollama, you can execute the following:</p><pre class="language-bash"><code class="language-bash"> ollama <span class="token parameter variable">--help</span></code></pre><p>It will give you the following output:</p><img class="center" src="/images/ollama-commands/02ollama-help.jpg" loading="lazy" title="Output of ollama --help showing all available Ollama commands" alt="Output of ollama --help showing all available Ollama commands"><p>You can also run <code>ollama --version</code> to check the version of Ollama when writing the version of Ollama is <code>0.5.7</code>. If you want help with a specific sub-command, you can add <code>--help</code> after the sub-command; for example: <code>ollama run --help</code> will give you the following output:</p><img class="center" src="/images/ollama-commands/03ollama-run-help.jpg" loading="lazy" title="Output of ollama run --help showing available flag and environment variables" alt="Output of ollama run --help showing available flag and environment variables"><p>Now that you know the basics, let’s look at some useful Ollama commands.</p><h3 id="ollama-serve" tabindex="-1">Ollama serve <a class="direct-link" href="#ollama-serve">#</a></h3><p>Ollama <code>serve</code> is the main command that starts the Ollama server. It can be configured with many environment variables, such as <code>OLLAMA_DEBUG</code> to enable or disable debugging, <code>OLLAMA_HOST</code> to specify the server's host, and <code>OLLAMA_MAX_QUEUE</code> to configure the maximum number of queued requests. To learn more about these environment variables, run <code>ollama serve --help</code>.</p><p>Ollama runs <a href="https://github.com/ollama/blob/main/server/routes.go#L27">Gin</a> (written in Go) as the underlying server to add an API layer to the downloaded (pulled) models. Both the CLI and any other services that need to use LLM inference will use the server started with <code>ollama serve</code>, which will give an output similar to the below:</p><img class="center" src="/images/ollama-commands/04ollama-serve.jpg" loading="lazy" title="Output of ollama serve showing applied environment variables" alt="Output of ollama serve showing applied environment variables"><p>The Gin server runs on port <code>11434</code> by default, so if you hit <code>http://localhost:11434/</code> on the browser of your choice (probably Chrome), you will see the text <code>Ollama is running</code>. The next part of this Ollama series will discuss the API in detail.</p><p>Given that the server is running, you will run a model next with <code>ollama run</code>.</p><h3 id="ollama-run" tabindex="-1">Ollama run <a class="direct-link" href="#ollama-run">#</a></h3><p>The Ollama <code>run</code> command runs an open model available in the Ollama models <a href="https://ollama.com/search">page</a>. It will pull (download) the model to your machine and then run it, exposing it via the API started with <code>ollama serve</code>. Like the previous part, you will run the Smollm2 135 million parameter because it will run on most machines with even less memory (like 512 MB), as the model is 271 MB.</p><p>To run Smollm2 135M parameters model, you can execute:</p><pre class="language-bash"><code class="language-bash">ollama run smollm2:135m</code></pre><p>It will result in something like the following:</p><img class="center" src="/images/ollama-commands/05ollama-run-smollm2.jpg" loading="lazy" title="Output of ollama run smollm2:145m on the CLI when the model is already downloaded" alt="Output of ollama run smollm2:145m on the CLI when the model is already downloaded"><p>If you ran the model for the first time, it would have downloaded and run, as seen in the last <a href="/blog/2025/02/what-is-ollama/">part</a> of this Ollama series. However, running an already pulled (downloaded) model runs quickly the second time.</p><p>If you type <code>/?</code> within the run command, you will see the help. You can set variables for the model like <code>num_ctx</code>, which can be used to configure the <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-specify-the-context-window-size">context window</a> of the model. For instance, you can type in <code>/set parameter num_ctx 8129</code> to set the context window to 8129 tokens.</p><p>You can also try <code>/show info</code>, and it will show you the model’s information like:</p><pre class="language-bash"><code class="language-bash"><span class="token operator">>></span><span class="token operator">></span> /show info<br>  Model<br>    architecture        llama      <br>    parameters          <span class="token number">134</span>.52M    <br>    context length      <span class="token number">8192</span>       <br>    embedding length    <span class="token number">576</span>        <br>    quantization        F16        <br><br>  Parameters<br>    stop    <span class="token string">"&lt;|im_start|>"</span>    <br>    stop    <span class="token string">"&lt;|im_end|>"</span>      <br><br>  System<br>    You are a helpful AI assistant named SmolLM, trained by Hugging Face    <br><br>  License<br>    Apache License               <br>    Version <span class="token number">2.0</span>, January <span class="token number">2004</span>    </code></pre><p>You can play with the other commands you run in the running model context. You can also chat with the model, asking it questions like <code>what is the speed of light?</code> gave me the following output with the smollm2 135M parameter model:</p><pre class="language-bash"><code class="language-bash"><span class="token operator">>></span><span class="token operator">></span> what is the speed of light?<br>The speed of light <span class="token keyword">in</span> space is approximately <span class="token number">299,792</span>,458 meters per second. This value is an approximation based on observations and calculations made using special relativity theory. While it<span class="token string">'s difficult to measure precisely with our current technology, scientists have used the latest methods to estimate this value for both magnitude (1) as well as relative motion (the speed of light squared). <br><br>It'</span>s worth noting that even <span class="token keyword">if</span> we can't directly calculate the exact speed of light <span class="token keyword">in</span> a vacuum using modern instruments and techniques, scientists often rely on estimates like those mentioned above.</code></pre><p>To exit the running model context, type <code>/bye</code> and return to the command line. Ollama run is a versatile command that executes prompts directly within the terminal, facilitating quick and efficient interactions with your models.</p><h3 id="ollama-list" tabindex="-1">Ollama list <a class="direct-link" href="#ollama-list">#</a></h3><p>The Ollma list command lists all the open models pulled (downloaded) from Ollama’s registry and saved to your machine. When I ran <code>ollama list</code> on my machine, I got the following output:</p><pre class="language-bash"><code class="language-bash"><mark class="highlight-line highlight-line-active">ollama list    </mark><br><span class="highlight-line">NAME                           ID                 SIZE      MODIFIED   </span><br><span class="highlight-line">deepseek-r1:8b    28f8fd6cdc67      <span class="token number">4.9</span> GB     <span class="token number">4</span> days ago    </span><br><span class="highlight-line">smollm2:135m      9077fe9d2ae1    <span class="token number">270</span> MB    <span class="token number">4</span> days ago    </span></code></pre><p>So, I have two models, <code>smollm2:135m</code> and <code>deepseek-r1:8b</code>, which are 270MB and 4.9 GB, respectively.</p><h3 id="ollama-pull" tabindex="-1">Ollama pull <a class="direct-link" href="#ollama-pull">#</a></h3><p>You can download other models from the Ollama registry on your machine using the <code>ollama pull</code> command. For instance, if you want to pull in Qwen 2.5 half a billion parameter model (398 MB), you can execute:</p><pre class="language-bash"><code class="language-bash">ollama pull qwen2.5:0.5b</code></pre><p>That will result in something like:</p><pre class="language-bash"><code class="language-bash"><mark class="highlight-line highlight-line-active">ollama pull qwen2.5:0.5b</mark><br><span class="highlight-line"></span><br><span class="highlight-line">pulling manifest</span><br><span class="highlight-line">pulling c5396e06af29<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕██████████████████████████▏ <span class="token number">397</span> MB</span><br><span class="highlight-line">pulling 66b9ea09bd5b<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕██████████████████████████▏   <span class="token number">68</span> B</span><br><span class="highlight-line">pulling eb4402837c78<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕██████████████████████████▏ <span class="token number">1.5</span> KB</span><br><span class="highlight-line">pulling 832dd9e00a68<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕██████████████████████████▏  <span class="token number">11</span> KB</span><br><span class="highlight-line">pulling 005f95c74751<span class="token punctuation">..</span>. <span class="token number">100</span>% ▕██████████████████████████▏  <span class="token number">490</span> B</span><br><span class="highlight-line">verifying sha256 digest</span><br><span class="highlight-line">writing manifest</span><br><span class="highlight-line">success</span></code></pre><p>It will take some minutes, depending on your internet speed. If you run <code>ollama list</code> after pulling the Qwen model, it will be listed too like below:</p><pre class="language-bash"><code class="language-bash"><mark class="highlight-line highlight-line-active">ollama list</mark><br><span class="highlight-line">NAME                   ID            SIZE      MODIFIED       </span><br><span class="highlight-line">qwen2.5:0.5b      a8b0c5157701       <span class="token number">397</span> MB    <span class="token number">50</span> seconds ago    </span><br><span class="highlight-line">deepseek-r1:8b    28f8fd6cdc67       <span class="token number">4.9</span> GB    <span class="token number">4</span> days ago        </span><br><span class="highlight-line">smollm2:135m      9077fe9d2ae1       <span class="token number">270</span> MB    <span class="token number">4</span> days ago    </span></code></pre><p>Similarly, running the Qwen model now will run directly rather than downloading and running it after downloading. You can also look at the CLI tab running <code>ollama serve</code> to see all the API calls these commands make in the background. Ollama pull seamlessly downloads and integrates pre-trained models from the vast Ollama model library to be used on your machine.</p><h3 id="ollama-ps" tabindex="-1">Ollama ps <a class="direct-link" href="#ollama-ps">#</a></h3><p>Like other <code>ps</code> commands that list processes, the <code>ollama ps</code> command will list running models. For this, you will first need to run a model; you can run the Qwen2 0.5 B parameters model with <code>ollama run qwen2.5:0.5b</code>. Then, in a new CLI tab, you can run <code>ollama ps</code>, which will give an output similar to the following:</p><pre class="language-bash"><code class="language-bash"><mark class="highlight-line highlight-line-active">ollama <span class="token function">ps</span></mark><br><span class="highlight-line">NAME            ID              SIZE      PROCESSOR    UNTIL              </span><br><span class="highlight-line">qwen2.5:0.5b    a8b0c5157701    <span class="token number">1.4</span> GB    <span class="token number">100</span>% GPU     <span class="token number">4</span> minutes from now  </span></code></pre><p>To exit the run context, type <code>/bye</code> to return to the command line.</p><h3 id="ollama-create" tabindex="-1">Ollama create <a class="direct-link" href="#ollama-create">#</a></h3><p>With the <code>ollama create</code> command, you can create a new variant of an existing open model. For example, you will create a new variant of the <code>smollm2:135m</code> parameter model with a context window of 16K, and the temperature (creativeness) is set to 0.1, which is significantly less creative. To do this, first, you will create a Model file named <code>Modelfile-smollm2-16k</code> in your current folder with the following content:</p><pre class="language-bash"><code class="language-bash">FROM smollm2:135m<br><br>PARAMETER temperature <span class="token number">0.2</span><br>PARAMETER num_ctx <span class="token number">16384</span></code></pre><p>Like Docker, you are saying to start from the <code>smollm2:135m</code>, set the <code>temperature</code> parameter to <code>0.2</code>, and set the context with the <code>num_ctx</code> parameter to <code>16384</code>.</p><p>Now, to create a new model named <code>smollm2:135m-16k-ctx</code> you will run the following command:</p><pre class="language-bash"><code class="language-bash">ollama create smollm2:135m-16k-ctx <span class="token parameter variable">-f</span> Modelfile-smollm2-16k</code></pre><p>It will create a new variant of the Smollm2 135 million parameter model following the instructions in the model file. If you run <code>ollama list</code>, you will see the new model on the list. Then, to run the new model, you can execute <code>ollama run smollm2:135m-16k-ctx</code> as seen below:</p><img class="center" src="/images/ollama-commands/06ollama-create.jpg" loading="lazy" title="Output of ollama create a new model with 0.2 temperature and context window of 16K tokens" alt="Output of ollama create a new model with 0.2 temperature and context window of 16K tokens"><p>In the running model context, where you can type <code>/?</code> for help, if you type in <code>/show info</code> you will see the following output:</p><pre class="language-bash"><code class="language-bash"><mark class="highlight-line highlight-line-active">/show info</mark><br><span class="highlight-line">  Model</span><br><span class="highlight-line">    architecture        llama      </span><br><span class="highlight-line">    parameters          <span class="token number">134</span>.52M    </span><br><span class="highlight-line">    context length      <span class="token number">8192</span>       </span><br><span class="highlight-line">    embedding length    <span class="token number">576</span>        </span><br><span class="highlight-line">    quantization        F16        </span><br><span class="highlight-line"></span><br><span class="highlight-line">  Parameters</span><br><span class="highlight-line">    temperature    <span class="token number">0.2</span>               </span><br><span class="highlight-line">    num_ctx        <span class="token number">16384</span>             </span><br><span class="highlight-line">    stop           <span class="token string">"&lt;|im_start|>"</span>    </span><br><span class="highlight-line">    stop           <span class="token string">"&lt;|im_end|>"</span>      </span><br><span class="highlight-line"></span><br><span class="highlight-line">  System</span><br><span class="highlight-line">    You are a helpful AI assistant named SmolLM, trained by Hugging Face    </span><br><span class="highlight-line"></span><br><span class="highlight-line">  License</span><br><span class="highlight-line">    Apache License               </span><br><span class="highlight-line">    Version <span class="token number">2.0</span>, January <span class="token number">2004</span></span></code></pre><p>This means the two parameters specified in the Model file, <code>temperature</code> and context window with <code>num_ctx</code>, are applied to the model. Because the temperature is set to a low value <code>0.2</code>, if you ask this model variant, <code>why is the sky blue? give me 1 sentence answer.</code> even 3 times consecutively; it will give you almost the same answer as seen below:</p><img class="center" src="/images/ollama-commands/07ollama-sky-blue.jpg" loading="lazy" title="Output of ollama why is the sky blue, same answer as the temperature is only 0.2" alt="Output of ollama why is the sky blue, same answer as the temperature is only 0.2"><p>Next, you will learn about some other Ollama commands.</p><h3 id="other-ollama-commands" tabindex="-1">Other Ollama commands <a class="direct-link" href="#other-ollama-commands">#</a></h3><p>If you can pull a model, you can <code>push</code> a model to the Ollama registry. For this, you will need an Ollama account and API keys to <a href="https://github.com/ollama/ollama/blob/main/docs/import.md#sharing-your-model-on-ollamacom">share your model</a> on Ollama.</p><p>Similarly, you can copy a model with <code>ollama cp</code> and remove a model with <code>ollama rm</code> followed by the model's name. You can also run <code>ollama show &lt;model-name&gt;</code> to see the configuration of the model; for example, <code>ollama show smollm2:135m</code> will show the following:</p><pre class="language-bash"><code class="language-bash"><mark class="highlight-line highlight-line-active">ollama show smollm2:135m </mark><br><span class="highlight-line">  Model</span><br><span class="highlight-line">    architecture        llama      </span><br><span class="highlight-line">    parameters          <span class="token number">134</span>.52M    </span><br><span class="highlight-line">    context length      <span class="token number">8192</span>       </span><br><span class="highlight-line">    embedding length    <span class="token number">576</span>        </span><br><span class="highlight-line">    quantization        F16        </span><br><span class="highlight-line"></span><br><span class="highlight-line">  Parameters</span><br><span class="highlight-line">    stop    <span class="token string">"&lt;|im_start|>"</span>    </span><br><span class="highlight-line">    stop    <span class="token string">"&lt;|im_end|>"</span>      </span><br><span class="highlight-line"></span><br><span class="highlight-line">  System</span><br><span class="highlight-line">    You are a helpful AI assistant named SmolLM, trained by Hugging Face    </span><br><span class="highlight-line"></span><br><span class="highlight-line">  License</span><br><span class="highlight-line">    Apache License               </span><br><span class="highlight-line">    Version <span class="token number">2.0</span>, January <span class="token number">2004</span>    </span></code></pre><p>The above output is the same as running <code>/show info</code> when the model runs within the CLI. As the new versions of Ollama are released, it may have new commands. To learn the list of Ollama commands, run <code>ollama --help</code> and find the available commands.</p><p>Ollama commands are similar to <a href="/blog/2022/05/docker-commands/">Docker commands</a>, like pull, push, ps, rm. In the case of Docker, it works with Docker images or containers, and for Ollama, it works with open LLM models.</p><p>In the next part of this Ollama series, you will learn about the Ollama APIs. The CLI also uses these APIs; you will learn more about them so that another system can use them for LLM inference.</p><h2 id="conclusion" tabindex="-1">Conclusion <a class="direct-link" href="#conclusion">#</a></h2><p>In this comprehensive guide, you explored a wide range of essential Ollama commands, From <code>ollama serve</code> to <code>ollama run</code>, and from <code>ollama pull</code> to <code>ollam create</code>. By mastering these Ollama commands, you'll be well-equipped to harness the full potential of this powerful and easy-to-use framework, opening up a world of possibilities for your projects and applications. Whether you're a seasoned developer or just starting your journey into AI, Ollama and its commands will undoubtedly be invaluable assets in your toolkit. Keep learning!</p></div><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2025-02-06" data-updated="true"><time class="entry-date" datetime="2025-02-06"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">06-Feb-2025 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">13 MIN READ</span></div><button type="button" id="share-button-bottom" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIconsBottom(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container-bottom" class="share-icons-bottom print:hidden hidden mx-auto" aria-label="Share this post"><div class="w-full flex items-center justify-center mx-auto gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=Ollama commands: How to use Ollama in the command line [Part 2] - &url=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2025/02/ollama-commands/" target="_blank" title="Share 'Ollama commands: How to use Ollama in the command line [Part 2]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></article><div class="mt-3"><a href="/blog/categories/ai/" title="AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">ai </a><a href="/blog/categories/gen-ai/" title="Gen AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">gen ai </a><a href="/blog/categories/ollama/" title="Ollama" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">ollama</a></div><section class="md:w-full mx-auto mt-4"><h3 class="comments font-heading">Comments</h3><div id="disqus_thread"></div><button id="disqus_trigger" class="font-ui text-blackText hover:border-darkAvocado border-blacktext hover:shadow-lg border-2 transition ease-in-out delay-100 px-3 py-2 pt-2 mt-2 rounded-md text-center block h-auto text-xl" onclick="load_disqus()">Post a Comment</button><div id="disqus_thread" aria-live="polite"></div></section></div></div><section class="md:w-full mx-auto mt-4 bg-lightAvocado p-4 rounded-lg pb-8"><div class="max-w-6xl mx-auto md:py-12 px-4"><h2 class="text-4xl text-darkAvocado font-heading font-regular flex-shrink-0 py-8 tracking-normal leading-[105%] md:leading-[105%]"><span class="text-lightGray italic">Related</span> <span class="text-blackText">Blogs</span></h2><div class="grid grid-cols-1 md:grid-cols-2 gap-8"><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/ollama-docker-compose/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to use Ollama and Open WebUI with Docker Compose [Part 4]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">11-Feb-2025 &nbsp;&nbsp;&nbsp; 11 min read</time><p class="text-xl text-gray font-ui">Learn how to use Ollama and Open WebUI inside Docker with Docker compose to run any open LLM and create your own mini ChatGPT.</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/ollama-api/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">Using Ollama APIs to generate responses and much more [Part 3]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">09-Feb-2025 &nbsp;&nbsp;&nbsp; 16 min read</time><p class="text-xl text-gray font-ui">Learn how to use Ollama APIs like generate, chat and more like list model, pull model, etc with cURL and Jq with useful examples</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/what-is-ollama/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">What is Ollama and how to use it: a quick guide [part 1]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">02-Feb-2025 &nbsp;&nbsp;&nbsp; 9 min read</time><p class="text-xl text-gray font-ui">Learn what Ollama is, its features and how to run it on your local machine with DeepSeek R1 and Smollm2 models</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/01/ollama-google-cloud-run/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">20-Jan-2025 &nbsp;&nbsp;&nbsp; 11 min read</time><p class="text-xl text-gray font-ui">Learn how to run and host Gemma 2:2b with Ollama on Google Cloud Run in this step-by-step tutorial. You can use Gemma with an API, too, using Ollama</p></div></div></div></section><section class="bg-avocado md:py-0 py-12 px-4"><div class="max-w-6xl mx-auto"><div class="md:hidden"><h2 class="mb-4 text-3xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-base text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-base text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full text-center">Follow on LinkedIn &nbsp;→</a></div><div class="hidden md:grid lg:hidden grid-cols-3 gap-8 items-center"><div class="z-10 col-span-2"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity">Follow on LinkedIn &nbsp;→</a></div><div class="flex items-center justify-end col-span-1"><img class="w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"></div></div><div class="hidden lg:grid grid-cols-2 gap-8 lg:gap-12 items-center justify-between h-full"><div class="z-10"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><p class="text-xl text-blackText font-body font-regular mb-6">A big thank you for supporting this ad. free, unobstructive (no overlays, no pop-ups) blog.</p></div><div class="flex items-center gap-12 w-full md:w-auto justify-end self-end"><img class="w-16 md:w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"> <a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full md:w-auto text-center md:text-left">Follow on LinkedIn &nbsp;→</a></div></div></div></section></div></div></div></main><footer role="contentinfo"><footer class="bg-avocado sm:px-4"><div class="max-w-6xl mx-auto flex flex-col md:flex-row sm:flex-col justify-between gap-2 items-center sm:items-center md:items-center py-10 font-nav text-lightGray text-base font-regular"><div class="pb-0 sm:pb-2 text-gray"><span class="pr-4">Copyright © 2026 Geshan Manandhar.</span></div><div class="pb-0 sm:pb-2"><ul class="flex"><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> LinkedIn</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.twitter.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Twitter</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://github.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Github</a></li></ul></div></div></footer></footer><script src="/js/all.min.js" defer="defer"></script></body></html>