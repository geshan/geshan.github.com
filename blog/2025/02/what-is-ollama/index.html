<!doctype html><html class="no-js" lang="en"><head><meta charset="utf-8"><meta name="author" content="Geshan Manandhar"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="Learn what Ollama is, its features and how to run it on your local machine with DeepSeek R1 and Smollm2 models"><meta name="keywords" content="what is ollama, ollama, ollama deepseek, ollama smollm2, ollama features, ollama models, ollama local machine, ollama quick guide"><meta name="p:domain_verify" content="e654c68562abebfa25c291f59d7d00e8"><meta property="og:type" content="website"><meta property="og:url" content="https://geshan.com.np/blog/2025/02/what-is-ollama/"><meta property="og:title" content="What is Ollama and how to use it: a quick guide [part 1]"><meta property="og:description" content="Learn what Ollama is, its features and how to run it on your local machine with DeepSeek R1 and Smollm2 models"><meta property="og:site_name" content="Geshan&#39;s Blog"><meta property="og:image" content="https://geshan.com.np/images/what-is-ollama/01what-is-ollama.jpg"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:widgets:new-embed-design" content="on"><meta name="twitter:site" content="@geshan"><meta name="twitter:creator" content="@geshan"><meta name="twitter:title" content="What is Ollama and how to use it: a quick guide [part 1]"><meta name="twitter:description" content="Learn what Ollama is, its features and how to run it on your local machine with DeepSeek R1 and Smollm2 models"><meta name="twitter:image:src" content="https://geshan.com.np/images/what-is-ollama/01what-is-ollama.jpg"><link rel="canonical" href="https://geshan.com.np/blog/2025/02/what-is-ollama/"><meta property="fb:pages" content="30717799226"><meta property="fb:app_id" content="106030259434380"><meta name="monetization" content="$ilp.uphold.com/aKHWpqhphm9f"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicons/favicon-16x16.png"><link href="/atom.xml" rel="alternate" title="Geshan&#39;s Blog" type="application/atom+xml"><title>What is Ollama and how to use it: a quick guide</title><link rel="preconnect" href="/" crossorigin><link rel="preload" href="/css/fonts.css" as="style"><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-500-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-pro/source-sans-pro-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="/css/fonts.css"><link rel="stylesheet" href="/css/tw-006.css"><link rel="alternate" href="/atom.xml" type="application/atom+xml" title="Geshan&#39;s Blog"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-P3NXCVQEPE"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-P3NXCVQEPE');</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWT2D9T');</script><script src="https://cdn.jsdelivr.net/npm/@statsig/js-client@3/build/statsig-js-client+session-replay+web-analytics.min.js?apikey=client-rYO7rX8WSUyyLg9pKJwymLxM71tCE0CKgxgtUj4akzK"></script><link rel="manifest" href="/manifest.json"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="application-name" content="Geshan.com.np"><meta name="apple-mobile-web-app-title" content="Geshan.com.np"><meta name="msapplication-starturl" content="/index.html"><meta name="theme-color" content="#6947E7"><script>if ("serviceWorker" in navigator) {
    navigator.serviceWorker.register("/sw.js").then(function(registration) {
        console.log('ServiceWorker registration successful with scope: ', registration.scope);
    }, function(err) {
        console.log('ServiceWorker registration failed: ', err);
    });
  }</script></head><body class="overflow-x-hidden font-ui flex flex-col min-h-screen"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWT2D9T" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><header role="banner"><div><nav class="bg-white" role="navigation"><div class="w-full md:max-w-6xl mx-auto py-4 px-4"><div class="flex items-center justify-between h-16"><div class="flex-shrink-0"><a href="/" class="flex items-center"><img class="h-12 w-12" src="/images/theme/new_logo.svg" alt="Geshan G"></a></div><div class="flex items-center gap-12"><div class="hidden sm:flex items-center gap-12"><a href="/posts/1/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Posts </a><a href="/about/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Newsletter</a></div><form class="search flex items-center" action="https://www.google.com/search" method="GET"><input type="hidden" name="sitesearch" value="geshan.com.np"><div class="relative"><input class="bg-gray-100 border border-neutralGray rounded-lg py-2 pl-10 pr-4 font-body text-sm focus:outline-none focus:ring-2 focus:ring-avocado focus:border-transparent w-40" type="text" name="q" placeholder="Search" label="search"> <svg class="absolute left-3 top-1/2 transform -translate-y-1/2 w-4 h-4 text-neutralGray" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/></svg></div></form><button type="button" class="sm:hidden inline-flex items-center justify-center p-2 rounded-md text-black focus:outline-none focus:ring-2 focus:ring-inset focus:ring-avocado" onclick="mobileView()" aria-controls="mobile-menu" aria-expanded="false"><span class="sr-only">Open main menu</span> <svg class="w-6 h-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button></div></div></div><div class="mobile-menu fixed inset-0 z-40 hidden sm:hidden bg-black bg-opacity-40" role="dialog" aria-modal="true"><div class="absolute inset-0" onclick="mobileView()" aria-hidden="true"></div><div class="mobile-menu-panel relative ml-auto flex h-full w-10/12 max-w-sm flex-col justify-start bg-white shadow-xl"><div class="flex items-center justify-between px-4 py-4 border-b border-gray-200"><div class="flex items-center gap-3"><img class="h-8 w-8" src="/images/theme/new_logo.svg" alt="Geshan G"> <span class="text-lg font-semibold text-textColor font-body">Geshan's Blog</span></div><button type="button" class="inline-flex items-center justify-center rounded-full p-2 text-gray-700 hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-avocado" onclick="mobileView()" aria-label="Close main menu"><svg class="h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg></button></div><nav class="overflow-y-auto px-4 py-6 space-y-2"><a href="/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Home </a><a href="/posts/1/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Posts </a><a href="/about/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Newsletter</a></nav><div class="px 4 py-4 border-t border-gray-200"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="rounded-lg bg-darkAvocado px-4 py-2 text-base font-regular font-body text-white hover:bg-avocado hover:text-black transition-colors w-2/3 mx-auto ml-4">Connect on LinkedIn</a></div></div></div></nav></div></header><main id="wrap" role="main" class="flex-grow md:px-0"><div id="content"><div class="row"><div><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Geshan&#39;s Blog",
    "alternativeHeadline": "What is Ollama and how to use it: a quick guide [part 1]",
    "image": "https://geshan.com.np/images/what-is-ollama/01what-is-ollama.jpg",
    "editor": "Geshan Manandhar",
    "genre": "AI",
    "keywords": "what is ollama, ollama, ollama deepseek, ollama smollm2, ollama features, ollama models, ollama local machine, ollama quick guide",
    "url": "https://geshan.com.np/blog/2025/02/what-is-ollama/",
    "datePublished": "2025-02-02",
    "dateCreated": "2025-02-02",
    "dateModified": "2025-02-02",
    "description": "Learn what Ollama is, its features and how to run it on your local machine with DeepSeek R1 and Smollm2 models",
    "articleBody": "The world of AI has been hyped for more than two years now since the release of ChatGPT in November 2022. New tools and technologies emerge daily, promising to revolutionize our work and lives. If you&#39;re looking to harness the power of large language models (LLMs) for personal projects or even professional applications, Ollama might be the key. In this post, you will learn what Ollama is, how to download, install, and use Ollama to run a small model, and then run DeepSeek R1 (the other super popular LLM); let’s get started.Table of contents #What is OllamaOllama blog post seriesOllama featuresPrivacy and offline accessModel managementSeamless installationLocal APICustomizationHardware accelerationAdding a user interfaceHow to run Ollama locallyRun DeepSeek R1 with OllamaWhere does Ollama store the modelsConclusionWhat is Ollama #Ollama is an open-source tool mainly written in Go lang (89%) that runs open LLMs on your local machine (or a server). It acts like a bridge between any open LLM and your machine, not only running them but also providing an API layer on top of them so that another application or service can use them.Ollama is a user-friendly and powerful software for running LLMs locally. It hides the complexities of LLMs, packaging them to be accessible and easily customizable with a model file. There are alternatives to Ollama, like vllm and aphrodite, but Ollama is surely the most popular one. Ollama provides a clean, user-friendly interface that allows you to interact directly with LLMs, tailoring the experience to your needs.Ollama blog post series #This post is the first part of a series of posts on Ollama. In this series, you will learn about Ollama, its features, how to install and run it on your local machine, and how to use it with different models.The part 2 of this series will cover Ollama commands. In this part, you will learn about the various commands you can use with Ollama to interact with LLMs. You will learn how to list models, run models, pull models, and more.Similarly in part 3, you will learn about Ollama APIs, which are used by the CLI and can be used by other systems to interact with the LLMs mainly for generating responses out of an open LLM.In part 4, you will learn how to run Ollama with Docker Compose. Docker is a popular containerization tool that allows you to run applications in a container. You will also add Open WebUI to have a web interface on top of LLMs running on Ollama.Ollama features #Below are some important features of Ollama:Privacy and offline access #One of the most important features of Ollama is privacy and offline access. You can run open models privately on your machine, even without internet access. This not only enables you to use an LLM (say, for code suggestions) on a plane but also keeps your data on your local machine. Your data and files can stay safe in your local machine, and other big tech companies do not see it or get to use it for other purposes like training an LLM. This is a big advantage of Ollama over other cloud-based LLM services which send your data to the cloud for processing and may use it for other purposes.Model management #Adding a new model to the library of local models is easy. You can pull a model with  some code .  In the next part of this series, you will learn about Ollama commands.Conclusion #Ollama is a game-changer for anyone working with LLMs. It simplifies the often daunting complexities of LLM interactions, making this powerful technology accessible to a much broader audience. Ollama&#39;s intuitive interface and user-friendly design make it the perfect tool for maximizing the power of LLMs and effortlessly incorporating them into your workflow.In this post you learned how to install Ollama and run Smollom2 and DeepSeek R1 models on it using the command line. You also found out where Ollama stores the downloaded models. Happy AI exploration!",
    "author": { "@type": "Person", "name": "Geshan Manandhar" },
    "publisher": {
      "@type": "Organization",
      "name": "Geshan Manandhar",
      "logo": { "@type": "ImageObject", "url": "https://geshan.com.np/images/favicons/favicon-32x32.png" }
    },
    "mainEntityOfPage": { "@type": "WebPage", "@id": "https://geshan.com.np/blog/2025/02/what-is-ollama/" }
  }</script><div class="progress-bar"></div><div class="max-w-6xl py-3 mx-auto grid-cols-12 grid gap-15 px-4 md:px-0 mb-20"><div class="col-span-12"><article class="md:w-full mx-auto"><header class="page-header text-center"><h1 class="font-semibold font-heading text-gray text-center mb-6 tracking-[-0.01em] leading-[105%] md:leading-[105%]">What is Ollama and how to use it: a quick guide [part 1]</h1><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2025-02-02" data-updated="true"><time class="entry-date" datetime="2025-02-02"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">02-Feb-2025 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">9 MIN READ</span></div><button type="button" id="share-button" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIcons(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container" class="share-icons print:hidden hidden" aria-label="Share this post"><div class="flex items-center justify-center gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=What is Ollama and how to use it: a quick guide [part 1] - &url=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></header><div class="entry-content clearfix font-body text-xl text-gray"><p>The world of AI has been hyped for more than two years now since the release of ChatGPT in November 2022. New tools and technologies emerge daily, promising to revolutionize our work and lives. If you're looking to harness the power of large language models (LLMs) for personal projects or even professional applications, Ollama might be the key. In this post, you will learn what Ollama is, how to download, install, and use Ollama to run a small model, and then run DeepSeek R1 (the other super popular LLM); let’s get started.</p><img class="center" src="/images/what-is-ollama/01what-is-ollama.jpg" title="What is Ollama and how to use it: a quick guide [part 1]" alt="What is Ollama and how to use it: a quick guide [part 1]"><h2 id="table-of-contents" tabindex="-1">Table of contents <a class="direct-link" href="#table-of-contents">#</a></h2><ul><li><a href="#what-is-ollama">What is Ollama</a></li><li><a href="#ollama-blog-post-series">Ollama blog post series</a></li><li><a href="#ollama-features">Ollama features</a><ul><li><a href="#privacy-and-offline-access">Privacy and offline access</a></li><li><a href="#model-management">Model management</a></li><li><a href="#seamless-installation">Seamless installation</a></li><li><a href="#local-api">Local API</a></li><li><a href="#customization">Customization</a></li><li><a href="#hardware-acceleration">Hardware acceleration</a></li><li><a href="#adding-a-user-interface">Adding a user interface</a></li></ul></li><li><a href="#how-to-run-ollama-locally">How to run Ollama locally</a><ul><li><a href="#run-deepseek-r1-with-ollama">Run DeepSeek R1 with Ollama</a></li><li><a href="#where-does-ollama-store-the-models">Where does Ollama store the models</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul><h2 id="what-is-ollama" tabindex="-1">What is Ollama <a class="direct-link" href="#what-is-ollama">#</a></h2><p>Ollama is an <a href="https://github.com/ollama/ollama">open-source</a> tool mainly written in Go lang (89%) that runs open LLMs on your local machine (or a server). It acts like a bridge between any open LLM and your machine, not only running them but also providing an API layer on top of them so that another application or service can use them.</p><p><a href="https://ollama.com/">Ollama</a> is a user-friendly and powerful software for running LLMs locally. It hides the complexities of LLMs, packaging them to be accessible and easily customizable with a <a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md">model file</a>. There are alternatives to Ollama, like <a href="https://github.com/vllm-project/vllm">vllm</a> and <a href="https://github.com/aphrodite-engine/aphrodite-engine">aphrodite</a>, but Ollama is surely the most popular one. Ollama provides a clean, user-friendly interface that allows you to interact directly with LLMs, tailoring the experience to your needs.</p><h2 id="ollama-blog-post-series" tabindex="-1">Ollama blog post series <a class="direct-link" href="#ollama-blog-post-series">#</a></h2><p>This post is the first part of a series of posts on Ollama. In this series, you will learn about Ollama, its features, how to install and run it on your local machine, and how to use it with different models.</p><p>The part 2 of this series will cover <a href="/blog/2025/02/ollama-commands/">Ollama commands</a>. In this part, you will learn about the various commands you can use with Ollama to interact with LLMs. You will learn how to list models, run models, pull models, and more.</p><p>Similarly in part 3, you will learn about <a href="/blog/2025/02/ollama-api/">Ollama APIs</a>, which are used by the CLI and can be used by other systems to interact with the LLMs mainly for generating responses out of an open LLM.</p><p>In part 4, you will learn how to run <a href="/blog/2025/02/ollama-docker-compose/">Ollama with Docker Compose</a>. Docker is a popular containerization tool that allows you to run applications in a container. You will also add Open WebUI to have a web interface on top of LLMs running on Ollama.</p><h2 id="ollama-features" tabindex="-1">Ollama features <a class="direct-link" href="#ollama-features">#</a></h2><p>Below are some important features of Ollama:</p><h3 id="privacy-and-offline-access" tabindex="-1">Privacy and offline access <a class="direct-link" href="#privacy-and-offline-access">#</a></h3><p>One of the most important features of Ollama is privacy and offline access. You can run open models privately on your machine, even without internet access. This not only enables you to use an LLM (say, for code suggestions) on a plane but also keeps your data on your local machine. Your data and files can stay safe in your local machine, and other big tech companies do not see it or get to use it for other purposes like training an LLM. This is a big advantage of Ollama over other cloud-based LLM services which send your data to the cloud for processing and may use it for other purposes.</p><h3 id="model-management" tabindex="-1">Model management <a class="direct-link" href="#model-management">#</a></h3><p>Adding a new model to the library of local models is easy. You can pull a model with <code>ollama pull</code> command and run it. At the time of writing, there are 150+ models you can pull and run locally, from DeepSeek R1 to Smollm, you can run popular models like Llama 3, Phi 4, Gemma 2, Mistral to code-specific models like Codeqwen, and Codestral. There is no need to navigate any complex format or dependencies or install any other software on your machine. You can pull a model and run it; if your system's resources support it, it will run easily.</p><h3 id="seamless-installation" tabindex="-1">Seamless installation <a class="direct-link" href="#seamless-installation">#</a></h3><p>As you will learn in the next section, Ollama's installation process is very user-friendly. Regardless of your Windows, Linux, or Mac operating system, you essentially run a command, and the Ollama CLI is available on your local machine. Thus, Ollama surely boasts a smooth and hassle-free installation and setup experience. With one command you can istall ollma CLI on a Linux or Mac operating system.</p><h3 id="local-api" tabindex="-1">Local API <a class="direct-link" href="#local-api">#</a></h3><p>Ollama has various commands, including the <code>ollama serve</code> command. This command starts a <a href="https://github.com/gin-gonic/gin">Gin server</a> expoing an <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">API</a> over all the LLM models available on your local system. This allows you to integrate LLMs into other applications and workflows. An API enables efficient communication between your application and LLMs. You can send prompts, get responses back, and exploit the full potential of LLMs. Using <a href="https://ollama.com/blog/structured-outputs">structured outputs</a>, you can even get the response from the LLMs in a predefined schema.</p><h3 id="customization" tabindex="-1">Customization <a class="direct-link" href="#customization">#</a></h3><p>With Ollama you can tweak parameters and extract the most value out of LLMs on your local machine. For instance you can change the <code>num_ctx</code> parameter and change the <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-specify-the-context-window-size">context window size</a> for a model with Ollama. You can also change the configs to fine-tune LLM parameters, adjust settings and change the model behaviour to better suits your needs. You can also use a <a href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md#parameter">modelfile</a> to set multiple parameters like temperature, seed, top K, top P and others.</p><h3 id="hardware-acceleration" tabindex="-1">Hardware acceleration <a class="direct-link" href="#hardware-acceleration">#</a></h3><p>Figuring out the resource and computational needs of the LLMs, Ollama can leverage the available hardware resources on your system including GPUs and CPUs. To check this you can run a <code>ollma ps</code> command and see the <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-tell-if-my-model-was-loaded-onto-the-gpu">resource</a> usages. Ollama makes sure the resources in your machine are utilized efficiently which enables you to run even large LLMs easily.</p><h3 id="adding-a-user-interface" tabindex="-1">Adding a user interface <a class="direct-link" href="#adding-a-user-interface">#</a></h3><p>Ollama is a command-line interface (CLI) aimed at advanced users. To use a graphical user interface with a chat, you can download and use <a href="https://openwebui.com/">open web ui</a> (previously known as Ollama Web UI). Open web UI is also <a href="https://github.com/open-webui/open-webui">open-source</a>. It claims to be an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline. You can read the official <a href="https://docs.openwebui.com/getting-started/quick-start/starting-with-ollama">docs</a> to get started with open web UI which plays along well with Ollama.</p><p>In the next seciton, you will learn how to install and use Ollama.</p><h2 id="how-to-run-ollama-locally" tabindex="-1">How to run Ollama locally <a class="direct-link" href="#how-to-run-ollama-locally">#</a></h2><p>Ollama is easy to install and use on any operating system, be it Windows, Linux or Mac. For this guide, you will install it on a Mac and run the <code>smollom2</code> model with 135 million parameters which is 271 MB. The <a href="https://ollama.com/library/smollm">smollom</a> 135 M parameters is 92 MB which is one of the smallest but still useful model.</p><p>To install Ollama on the operating system of your choice, you can navigate to the official Ollama <a href="https://ollama.com/download">download</a> page. It looks as follow:</p><img class="center" src="/images/what-is-ollama/02download-ollama.jpg" loading="lazy" title="Official page to downlaod Ollama for Windows, Mac or Linux" alt="Official page to downlaod Ollama for Windows, Mac or Linux"><p>As I am using a Mac I can download the installer from that page or run the following command to get Ollama CLI installed on a Mac:</p><pre class="language-bash"><code class="language-bash">brew <span class="token function">install</span> ollama</code></pre><p>It will install Ollama and all its related dependences and end like the below if all goes well:</p><img class="center" src="/images/what-is-ollama/03brew-install-ollama-output.jpg" loading="lazy" title="Final successful output of brew install ollama on a Mac" alt="Final successful output of brew install ollama on a Mac"><p>If you installed it with brew you will need to run <code>ollama serve</code>. You will see the ollama variables and resources.<br>Then you can run <code>ollama –version</code> in a different CLI tab. You will see something like the bleow:</p><pre class="language-bash"><code class="language-bash"> ➜  ollama <span class="token parameter variable">--version</span><br>ollama version is <span class="token number">0.5</span>.7</code></pre><p>At the time of writing the latest version of Ollama is <code>0.5.7</code>. As this point, because ollama is just installed you will not have any models. To check that you can run <code>ollama list</code> which will show an empty list.</p><p>Now, to install and run the <a href="https://ollama.com/library/smollm2:135m">smollom2</a> 135 million parameter model you will execute the following on your command line:</p><pre class="language-bash"><code class="language-bash">ollama run smollm2:135m</code></pre><p>Depending on your internet speed to download the 271MB model, it will take some time and show your the following output, where you can type your first question/promot to the model like<br><code>why is the sky blue? give the shortest possible answer</code>:</p><img class="center" src="/images/what-is-ollama/04ollama-run-smollm2.jpg" loading="lazy" title="Running Smollm2 135 million parameters model (271 MB) with Ollama" alt="Running Smollm2 135 million parameters model (271 MB) with Ollama"><p>You can play around and send it prompts or questions like <code>who are you?</code>. It will give the answers. If you write <code>/?</code> you will see the help menu as follows:</p><img class="center" src="/images/what-is-ollama/05ollama-bye.jpg" loading="lazy" title="Exit the Ollama prompt of running model with /bye" alt="Exit the Ollama prompt of running model with /bye"><p>You can type in <code>/bye</code> to stop the running model and get out of its prompt. Congrats! You have successfully run a relatively smaller LLM (pun intended) on your machine. As it is a smaller model with only 135 million parameters (271 MB) it might not perform many tasks well. So in the next section you will run the popular DeepSeek R1 8 billion parameter model.</p><h3 id="run-deepseek-r1-with-ollama" tabindex="-1">Run DeepSeek R1 with Ollama <a class="direct-link" href="#run-deepseek-r1-with-ollama">#</a></h3><p>To run DeepSeek R1 8 billion parameter model you can run the following command:</p><pre class="language-bash"><code class="language-bash">ollama run deepseek-r1:8b</code></pre><p>Keep in mind, you are downloading almost 4.9 GB of data, so depending on your internet speed it might take minutes (or hours, I can’t say). With Ollama you can try many open models, have a look at their <a href="https://ollama.com/search">models</a> page and you can choose which model you would like to run on your local machine. Make sure that you have enough resources, memory, CPU and disk space to run a really large model. For iinstance the DeepSeek R1 671B parameters model is 404 GB which will surely not run a consumer grade laptop.</p><p>In my case, after a few minutes of watiing the <code>deepseek-r1:8b</code> model was downloaded and it ran. You can run the same question/prompt of <code>why is the sky blue? give the shortest possible answer</code> and see the response from the model as seen below:</p><img class="center" src="/images/what-is-ollama/06ollama-deepseek-r1.jpg" loading="lazy" title="Running Deepseek R1 8 billion parameters model (4.9 GB) with Ollama" alt="Running Deepseek R1 8 billion parameters model (4.9 GB) with Ollama"><p>As you can see above, DeepSeek R1 (R for reasoning) apparently “thinks” and then gives back a response. Thats is something new. To stop the model and exit the chat you can type <code>/bye</code> and then get back to the bash.</p><h3 id="where-does-ollama-store-the-models" tabindex="-1">Where does Ollama store the models <a class="direct-link" href="#where-does-ollama-store-the-models">#</a></h3><p>Depending on your operationg system and how you installed Ollama it may vary. In my case, on a Mac it was stored in the user directory at <code>~/.ollama/models</code> as you can see below with the command <code>ls -al ~/.ollama/models</code>:</p><img class="center" src="/images/what-is-ollama/07ollama-model-storage.jpg" loading="lazy" title="Ollama stores models at ~/.ollama/models" alt="Ollama stores models at ~/.ollama/models"><p>Ollama store the models at <code>~/.ollama/models</code> on Linux machines as well. It can be changed by setting an <a href="https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux">environment variable</a> called <code>OLLAMA_MODELS</code>. In the next part of this series, you will learn about <a href="/blog/2025/02/ollama-commands/">Ollama commands</a>.</p><h2 id="conclusion" tabindex="-1">Conclusion <a class="direct-link" href="#conclusion">#</a></h2><p>Ollama is a game-changer for anyone working with LLMs. It simplifies the often daunting complexities of LLM interactions, making this powerful technology accessible to a much broader audience. Ollama's intuitive interface and user-friendly design make it the perfect tool for maximizing the power of LLMs and effortlessly incorporating them into your workflow.</p><p>In this post you learned how to install Ollama and run Smollom2 and DeepSeek R1 models on it using the command line. You also found out where Ollama stores the downloaded models. Happy AI exploration!</p></div><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2025-02-02" data-updated="true"><time class="entry-date" datetime="2025-02-02"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">02-Feb-2025 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">9 MIN READ</span></div><button type="button" id="share-button-bottom" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIconsBottom(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container-bottom" class="share-icons-bottom print:hidden hidden mx-auto" aria-label="Share this post"><div class="w-full flex items-center justify-center mx-auto gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=What is Ollama and how to use it: a quick guide [part 1] - &url=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2025/02/what-is-ollama/" target="_blank" title="Share 'What is Ollama and how to use it: a quick guide [part 1]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></article><div class="mt-3"><a href="/blog/categories/ai/" title="AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">ai </a><a href="/blog/categories/gen-ai/" title="Gen AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">gen ai </a><a href="/blog/categories/ollama/" title="Ollama" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">ollama</a></div><section class="md:w-full mx-auto mt-4"><h3 class="comments font-heading">Comments</h3><div id="disqus_thread"></div><button id="disqus_trigger" class="font-ui text-blackText hover:border-darkAvocado border-blacktext hover:shadow-lg border-2 transition ease-in-out delay-100 px-3 py-2 pt-2 mt-2 rounded-md text-center block h-auto text-xl" onclick="load_disqus()">Post a Comment</button><div id="disqus_thread" aria-live="polite"></div></section></div></div><section class="md:w-full mx-auto mt-4 bg-lightAvocado p-4 rounded-lg pb-8"><div class="max-w-6xl mx-auto md:py-12 px-4"><h2 class="text-4xl text-darkAvocado font-heading font-regular flex-shrink-0 py-8 tracking-normal leading-[105%] md:leading-[105%]"><span class="text-lightGray italic">Related</span> <span class="text-blackText">Blogs</span></h2><div class="grid grid-cols-1 md:grid-cols-2 gap-8"><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/ollama-docker-compose/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to use Ollama and Open WebUI with Docker Compose [Part 4]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">11-Feb-2025 &nbsp;&nbsp;&nbsp; 11 min read</time><p class="text-xl text-gray font-ui">Learn how to use Ollama and Open WebUI inside Docker with Docker compose to run any open LLM and create your own mini ChatGPT.</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/ollama-api/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">Using Ollama APIs to generate responses and much more [Part 3]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">09-Feb-2025 &nbsp;&nbsp;&nbsp; 16 min read</time><p class="text-xl text-gray font-ui">Learn how to use Ollama APIs like generate, chat and more like list model, pull model, etc with cURL and Jq with useful examples</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/02/ollama-commands/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">Ollama commands: How to use Ollama in the command line [Part 2]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">06-Feb-2025 &nbsp;&nbsp;&nbsp; 13 min read</time><p class="text-xl text-gray font-ui">Learn about the important Ollama commands to run Ollama on your local machine with Smollm2 and Qwen 2.5 models</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/01/ollama-google-cloud-run/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to run (any) open LLM with Ollama on Google Cloud Run [Step-by-step]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">20-Jan-2025 &nbsp;&nbsp;&nbsp; 11 min read</time><p class="text-xl text-gray font-ui">Learn how to run and host Gemma 2:2b with Ollama on Google Cloud Run in this step-by-step tutorial. You can use Gemma with an API, too, using Ollama</p></div></div></div></section><section class="bg-avocado md:py-0 py-12 px-4"><div class="max-w-6xl mx-auto"><div class="md:hidden"><h2 class="mb-4 text-3xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-base text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-base text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full text-center">Follow on LinkedIn &nbsp;→</a></div><div class="hidden md:grid lg:hidden grid-cols-3 gap-8 items-center"><div class="z-10 col-span-2"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity">Follow on LinkedIn &nbsp;→</a></div><div class="flex items-center justify-end col-span-1"><img class="w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"></div></div><div class="hidden lg:grid grid-cols-2 gap-8 lg:gap-12 items-center justify-between h-full"><div class="z-10"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><p class="text-xl text-blackText font-body font-regular mb-6">A big thank you for supporting this ad. free, unobstructive (no overlays, no pop-ups) blog.</p></div><div class="flex items-center gap-12 w-full md:w-auto justify-end self-end"><img class="w-16 md:w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"> <a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full md:w-auto text-center md:text-left">Follow on LinkedIn &nbsp;→</a></div></div></div></section></div></div></div></main><footer role="contentinfo"><footer class="bg-avocado sm:px-4"><div class="max-w-6xl mx-auto flex flex-col md:flex-row sm:flex-col justify-between gap-2 items-center sm:items-center md:items-center py-10 font-nav text-lightGray text-base font-regular"><div class="pb-0 sm:pb-2 text-gray"><span class="pr-4">Copyright © 2026 Geshan Manandhar.</span></div><div class="pb-0 sm:pb-2"><ul class="flex"><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> LinkedIn</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.twitter.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Twitter</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://github.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Github</a></li></ul></div></div></footer></footer><script src="/js/all.min.js" defer="defer"></script></body></html>