<!doctype html><html class="no-js" lang="en"><head><meta charset="utf-8"><meta name="author" content="Geshan Manandhar"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="A tutorial showing how to run Smollm2 with Docker Model Runner and Docker Compose to work with a simple Node.js/Express.js chat application"><meta name="keywords" content="docker model runner, docker model runner smollm2, docker model runner compose, docker model runner docker compose, docker model runner node.js app, docker model runner express.js, docker model runner chat app"><meta name="p:domain_verify" content="e654c68562abebfa25c291f59d7d00e8"><meta property="og:type" content="website"><meta property="og:url" content="https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/"><meta property="og:title" content="How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]"><meta property="og:description" content="A tutorial showing how to run Smollm2 with Docker Model Runner and Docker Compose to work with a simple Node.js/Express.js chat application"><meta property="og:site_name" content="Geshan&#39;s Blog"><meta property="og:image" content="https://geshan.com.np/images/docker-model-runner-docker-compose/01docker-model-runner-docker-compose.jpg"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:widgets:new-embed-design" content="on"><meta name="twitter:site" content="@geshan"><meta name="twitter:creator" content="@geshan"><meta name="twitter:title" content="How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]"><meta name="twitter:description" content="A tutorial showing how to run Smollm2 with Docker Model Runner and Docker Compose to work with a simple Node.js/Express.js chat application"><meta name="twitter:image:src" content="https://geshan.com.np/images/docker-model-runner-docker-compose/01docker-model-runner-docker-compose.jpg"><link rel="canonical" href="https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/"><meta property="fb:pages" content="30717799226"><meta property="fb:app_id" content="106030259434380"><meta name="monetization" content="$ilp.uphold.com/aKHWpqhphm9f"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicons/favicon-16x16.png"><link href="/atom.xml" rel="alternate" title="Geshan&#39;s Blog" type="application/atom+xml"><title>How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]</title><link rel="preconnect" href="/" crossorigin><link rel="preload" href="/css/fonts.css" as="style"><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-4/source-serif-4-latin-500-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="/fonts/source-sans-pro/source-sans-pro-latin-400-normal.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="/css/fonts.css"><link rel="stylesheet" href="/css/tw-006.css"><link rel="alternate" href="/atom.xml" type="application/atom+xml" title="Geshan&#39;s Blog"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin><script async src="https://www.googletagmanager.com/gtag/js?id=G-P3NXCVQEPE"></script><script>window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());gtag('config', 'G-P3NXCVQEPE');</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-TWT2D9T');</script><script src="https://cdn.jsdelivr.net/npm/@statsig/js-client@3/build/statsig-js-client+session-replay+web-analytics.min.js?apikey=client-rYO7rX8WSUyyLg9pKJwymLxM71tCE0CKgxgtUj4akzK"></script><link rel="manifest" href="/manifest.json"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="application-name" content="Geshan.com.np"><meta name="apple-mobile-web-app-title" content="Geshan.com.np"><meta name="msapplication-starturl" content="/index.html"><meta name="theme-color" content="#6947E7"><script>if ("serviceWorker" in navigator) {
    navigator.serviceWorker.register("/sw.js").then(function(registration) {
        console.log('ServiceWorker registration successful with scope: ', registration.scope);
    }, function(err) {
        console.log('ServiceWorker registration failed: ', err);
    });
  }</script></head><body class="overflow-x-hidden font-ui flex flex-col min-h-screen"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TWT2D9T" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><header role="banner"><div><nav class="bg-white" role="navigation"><div class="w-full md:max-w-6xl mx-auto py-4 px-4"><div class="flex items-center justify-between h-16"><div class="flex-shrink-0"><a href="/" class="flex items-center"><img class="h-12 w-12" src="/images/theme/new_logo.svg" alt="Geshan G"></a></div><div class="flex items-center gap-12"><div class="hidden sm:flex items-center gap-12"><a href="/posts/1/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Posts </a><a href="/about/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="uppercase text-base font-ui font-medium text-gray hover:text-midgrey transition-colors" aria-current="page">Newsletter</a></div><form class="search flex items-center" action="https://www.google.com/search" method="GET"><input type="hidden" name="sitesearch" value="geshan.com.np"><div class="relative"><input class="bg-gray-100 border border-neutralGray rounded-lg py-2 pl-10 pr-4 font-body text-sm focus:outline-none focus:ring-2 focus:ring-avocado focus:border-transparent w-40" type="text" name="q" placeholder="Search" label="search"> <svg class="absolute left-3 top-1/2 transform -translate-y-1/2 w-4 h-4 text-neutralGray" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"/></svg></div></form><button type="button" class="sm:hidden inline-flex items-center justify-center p-2 rounded-md text-black focus:outline-none focus:ring-2 focus:ring-inset focus:ring-avocado" onclick="mobileView()" aria-controls="mobile-menu" aria-expanded="false"><span class="sr-only">Open main menu</span> <svg class="w-6 h-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"/></svg></button></div></div></div><div class="mobile-menu fixed inset-0 z-40 hidden sm:hidden bg-black bg-opacity-40" role="dialog" aria-modal="true"><div class="absolute inset-0" onclick="mobileView()" aria-hidden="true"></div><div class="mobile-menu-panel relative ml-auto flex h-full w-10/12 max-w-sm flex-col justify-start bg-white shadow-xl"><div class="flex items-center justify-between px-4 py-4 border-b border-gray-200"><div class="flex items-center gap-3"><img class="h-8 w-8" src="/images/theme/new_logo.svg" alt="Geshan G"> <span class="text-lg font-semibold text-textColor font-body">Geshan's Blog</span></div><button type="button" class="inline-flex items-center justify-center rounded-full p-2 text-gray-700 hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-avocado" onclick="mobileView()" aria-label="Close main menu"><svg class="h-6 w-6" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"/></svg></button></div><nav class="overflow-y-auto px-4 py-6 space-y-2"><a href="/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Home </a><a href="/posts/1/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Posts </a><a href="/about/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">About Me </a><a href="https://newsletter.geshan.com.np/" class="block rounded-lg px-3 py-2 text-base font-medium font-ui text-black hover:bg-lightAvocado" aria-current="page">Newsletter</a></nav><div class="px 4 py-4 border-t border-gray-200"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="rounded-lg bg-darkAvocado px-4 py-2 text-base font-regular font-body text-white hover:bg-avocado hover:text-black transition-colors w-2/3 mx-auto ml-4">Connect on LinkedIn</a></div></div></div></nav></div></header><main id="wrap" role="main" class="flex-grow md:px-0"><div id="content"><div class="row"><div><script type="application/ld+json">{
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Geshan&#39;s Blog",
    "alternativeHeadline": "How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]",
    "image": "https://geshan.com.np/images/docker-model-runner-docker-compose/01docker-model-runner-docker-compose.jpg",
    "editor": "Geshan Manandhar",
    "genre": "AI",
    "keywords": "docker model runner, docker model runner smollm2, docker model runner compose, docker model runner docker compose, docker model runner node.js app, docker model runner express.js, docker model runner chat app",
    "url": "https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/",
    "datePublished": "2026-01-25",
    "dateCreated": "2026-01-25",
    "dateModified": "2026-01-25",
    "description": "A tutorial showing how to run Smollm2 with Docker Model Runner and Docker Compose to work with a simple Node.js/Express.js chat application",
    "articleBody": "You can run open models with other apps, such as Ollama. Docker Model Runner shines when you want to connect your application’s Docker container with an open model. It feels more native to Docker to define both the application and the model in a single Docker Compose file. You will learn to do so in this tutorial with a demo app built with Node.js that talks to Smollm2, defined in a Docker Compose file. Let’s get going!Table of contents #PrerequisitesSettings for the API on Docker DesktopThe demo Node applicationCode for the Node appDockerfile for Node appDocker compose file with Node app and Smollm2 modelRunning the app with Docker ComposeConclusionPrerequisites #In this part, similar to part 1 about Docker Model Runner, you will need Docker Desktop installed with Docker Model Runner available. You will also need a decent hardware configuration to run the models, especially the ones with billions of parameters. On top of that, the following things will also be needed:Docker compose installed on your machine. Compose is bundled with Docker Desktop if it is not installed, please install it.A general idea of how Docker Compose works and how services communicate in Docker Compose would be good to know. You can get a refresher with this Docker Compose tutorialGiven that, you can jump to the next section to configure the AI settings for Desktop.Settings for the API on Docker Desktop #Any open model you pull and run can expose the APIs for chat completion and other functionalities. For this API to be accessible from your local machine or other containers (inside or outside a docker compose set up), you will need to  some code  app is as follows:app.post(&#39;/api/chat&#39;, async (req, res) =&gt; {    const { message } = req.body;        // Special command for model info    if (message === &#39;!modelinfo&#39;) {        return res.json({ model: getModelName() });    }        try {        const response = await callLLMAPI(message);        return res.json({ response });    } catch (error) {        console.error(&#39;Error calling LLM API:&#39;, error.message);        return res.status(500).json({ error: &#39;Failed to get response from LLM&#39; });    }});// Call the LLM APIasync function callLLMAPI(userMessage) {    const chatRequest = {        model: getModelName(),        messages: [            {                role: &quot;system&quot;,                content: &quot;You are a helpful assistant.&quot;            },            {                role: &quot;user&quot;,                content: userMessage            }        ]    };        try {        const response = await axios.post(            getLLMEndpoint(),            chatRequest,            {                headers: { &#39;Content-Type&#39;: &#39;application/json&#39; },                timeout: 30000 // 30 seconds            }        );                if (response.data &amp;amp;&amp;amp; response.data.choices &amp;amp;&amp;amp; response.data.choices.length &gt; 0) {            return response.data.choices[0].message.content.trim();        }                throw new Error(&#39;No response choices returned from API&#39;);    } catch (error) {        if (error.response) {            throw new Error(`API returned status code ${error.response.status}: ${JSON.stringify(error.response.data)}`);        }        throw error;    }}The above code defines an Express.js API endpoint that sends user messages to a Large Language Model (LLM), which will be Smollm2 in the example, and returns the model’s reply.The  some code , it will not be running Smollm2 anymore.In my experience, when running AI models locally, they can start to eat up CPU resources. So it is best to unload the model when you are done using it.Conclusion #In this post, you built on part 1, the introduction to Docker Model Runner, by connecting a simple Node.js (Express.js) chat app to the Smollm2 model using Docker Compose. You saw the code for the chat app, then a simple Dockerfile to run a Node.js app, and finally the Docker Compose file that connects the app to the open model (SmolM2 in this case). Then you ran it and saw it in action. Keep Learning!",
    "author": { "@type": "Person", "name": "Geshan Manandhar" },
    "publisher": {
      "@type": "Organization",
      "name": "Geshan Manandhar",
      "logo": { "@type": "ImageObject", "url": "https://geshan.com.np/images/favicons/favicon-32x32.png" }
    },
    "mainEntityOfPage": { "@type": "WebPage", "@id": "https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" }
  }</script><div class="progress-bar"></div><div class="max-w-6xl py-3 mx-auto grid-cols-12 grid gap-15 px-4 md:px-0 mb-20"><div class="col-span-12"><article class="md:w-full mx-auto"><header class="page-header text-center"><h1 class="font-semibold font-heading text-gray text-center mb-6 tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]</h1><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2026-01-25" data-updated="true"><time class="entry-date" datetime="2026-01-25"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">25-Jan-2026 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">12 MIN READ</span></div><button type="button" id="share-button" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIcons(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container" class="share-icons print:hidden hidden" aria-label="Share this post"><div class="flex items-center justify-center gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2] - &url=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></header><div class="entry-content clearfix font-body text-xl text-gray"><p>You can run open models with other apps, such as <a href="/blog/2025/02/what-is-ollama/">Ollama</a>. Docker Model Runner shines when you want to connect your application’s Docker container with an open model. It feels more native to Docker to define both the application and the model in a single Docker Compose file. You will learn to do so in this tutorial with a demo app built with Node.js that talks to Smollm2, defined in a Docker Compose file. Let’s get going!</p><img class="center" src="/images/docker-model-runner-docker-compose/01docker-model-runner-docker-compose.jpg" title="Docker Model Runner: with Docker compose to run a Node.js app with Smollm2" alt="Docker Model Runner: with Docker compose to run a Node.js app with Smollm2" fetchpriority="high"><h2 id="table-of-contents" tabindex="-1">Table of contents <a class="direct-link" href="#table-of-contents">#</a></h2><ul><li><a href="#prerequisites">Prerequisites</a></li><li><a href="#settings-for-the-api-on-docker-desktop">Settings for the API on Docker Desktop</a></li><li><a href="#the-demo-node-application">The demo Node application</a><ul><li><a href="#code-for-the-node-app">Code for the Node app</a></li><li><a href="#dockerfile-for-node-app">Dockerfile for Node app</a></li><li><a href="#docker-compose-file-with-node-app-and-smollm2-model">Docker compose file with Node app and Smollm2 model</a></li><li><a href="#running-the-app-with-docker-compose">Running the app with Docker Compose</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul><h2 id="prerequisites" tabindex="-1">Prerequisites <a class="direct-link" href="#prerequisites">#</a></h2><p>In this part, similar to part 1 about <a href="/blog/2026/01/docker-model-runner/">Docker Model Runner</a>, you will need Docker Desktop installed with Docker Model Runner available. You will also need a decent hardware configuration to run the models, especially the ones with billions of parameters. On top of that, the following things will also be needed:</p><ul><li>Docker compose installed on your machine. Compose is bundled with Docker Desktop if it is not installed, please <a href="https://docs.docker.com/compose/install/">install</a> it.</li><li>A general idea of how Docker Compose works and how services communicate in Docker Compose would be good to know. You can get a refresher with this <a href="/blog/2024/04/docker-compose-tutorial/">Docker Compose tutorial</a></li></ul><p>Given that, you can jump to the next section to configure the AI settings for Desktop.</p><h2 id="settings-for-the-api-on-docker-desktop" tabindex="-1">Settings for the API on Docker Desktop <a class="direct-link" href="#settings-for-the-api-on-docker-desktop">#</a></h2><p>Any open model you pull and run can expose the APIs for chat completion and other functionalities. For this API to be accessible from your local machine or other containers (inside or outside a docker compose set up), you will need to <code>Enable host-side TCP support</code> when Docker Model Runner (DMR) is enabled.</p><p>To do this, you can follow the steps below:</p><ol><li>Open Docker Desktop</li><li>Click on the gear icon (⚙️) on the top right to show the settings of Docker Desktop</li><li>Then, click <code>AI</code> on the left sidebar</li><li>After that, make sure <code>Enable Docker Model Runner</code> is checked</li><li>Also confirm that the <code>Enable host-side TCP support</code> checkbox is also checked</li><li>Then click <code>Apply</code> as seen below:</li></ol><img class="center" src="/images/docker-model-runner-docker-compose/02docker-desktop-ai-models-settings.jpg" loading="lazy" title="Docker Desktop AI modles settings for Docker Model Runner" alt="Docker Desktop AI modles settings for Docker Model Runner"><p>By default, it will use port number <code>12343</code>, adjust <code>CORS</code> settings if you need to. In the next section, you will learn about the demo Node.js application you will use to chat with the Smollm2 model.</p><h2 id="the-demo-node-application" tabindex="-1">The demo Node application <a class="direct-link" href="#the-demo-node-application">#</a></h2><p>For this tutorial, rather than writing a completely new application, you will reuse the <a href="https://github.com/docker/hello-genai/tree/main/node-genai">Node.js version</a> of the <a href="https://github.com/docker/hello-genai/">hello-genai</a> open-source code from Docker.</p><p>You can use any open model like <a href="https://hub.docker.com/r/ai/gemma3">Gemma 3</a> or <a href="https://hub.docker.com/r/ai/ministral3">Mistral 3</a> or even <a href="https://hub.docker.com/r/ai/functiongemma">Gemma function</a> if you want to build AI Agents. For this guide, you will use Smollm2's default variant with 360 M parameters as it general purpose and small enough to run on most machines. Next, you will see a snippet of the Node.js Express ap,p which calls the Smollm2 model.</p><h3 id="code-for-the-node-app" tabindex="-1">Code for the Node app <a class="direct-link" href="#code-for-the-node-app">#</a></h3><p>The demo application connects a Node.js Express app to Smollm2, defined as a <code>model</code> in the Docker Compose file. The app is reused from Docker's open-source <a href="https://github.com/docker/hello-genai">hello-genai</a> repository. I have taken the <a href="https://github.com/docker/hello-genai/tree/main/node-genai">node-genai</a> app and modified it a bit. It is a simple chat interface used to send prompts to the model and display the response on a webpage. There is a screenshot if this in action in the later section.</p><p>Main part of the code for the Express.js app in the <code>node-genai</code> app is as follows:</p><pre class="language-js"><code class="language-js">app<span class="token punctuation">.</span><span class="token function">post</span><span class="token punctuation">(</span><span class="token string">'/api/chat'</span><span class="token punctuation">,</span> <span class="token keyword">async</span> <span class="token punctuation">(</span><span class="token parameter">req<span class="token punctuation">,</span> res</span><span class="token punctuation">)</span> <span class="token operator">=></span> <span class="token punctuation">{</span><br>    <span class="token keyword">const</span> <span class="token punctuation">{</span> message <span class="token punctuation">}</span> <span class="token operator">=</span> req<span class="token punctuation">.</span>body<span class="token punctuation">;</span><br>    <br>    <span class="token comment">// Special command for model info</span><br>    <span class="token keyword">if</span> <span class="token punctuation">(</span>message <span class="token operator">===</span> <span class="token string">'!modelinfo'</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>        <span class="token keyword">return</span> res<span class="token punctuation">.</span><span class="token function">json</span><span class="token punctuation">(</span><span class="token punctuation">{</span> <span class="token literal-property property">model</span><span class="token operator">:</span> <span class="token function">getModelName</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><br>    <span class="token punctuation">}</span><br>    <br>    <span class="token keyword">try</span> <span class="token punctuation">{</span><br>        <span class="token keyword">const</span> response <span class="token operator">=</span> <span class="token keyword">await</span> <span class="token function">callLLMAPI</span><span class="token punctuation">(</span>message<span class="token punctuation">)</span><span class="token punctuation">;</span><br>        <span class="token keyword">return</span> res<span class="token punctuation">.</span><span class="token function">json</span><span class="token punctuation">(</span><span class="token punctuation">{</span> response <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><br>    <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span>error<span class="token punctuation">)</span> <span class="token punctuation">{</span><br>        console<span class="token punctuation">.</span><span class="token function">error</span><span class="token punctuation">(</span><span class="token string">'Error calling LLM API:'</span><span class="token punctuation">,</span> error<span class="token punctuation">.</span>message<span class="token punctuation">)</span><span class="token punctuation">;</span><br>        <span class="token keyword">return</span> res<span class="token punctuation">.</span><span class="token function">status</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">json</span><span class="token punctuation">(</span><span class="token punctuation">{</span> <span class="token literal-property property">error</span><span class="token operator">:</span> <span class="token string">'Failed to get response from LLM'</span> <span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><br>    <span class="token punctuation">}</span><br><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">;</span><br><br><span class="token comment">// Call the LLM API</span><br><span class="token keyword">async</span> <span class="token keyword">function</span> <span class="token function">callLLMAPI</span><span class="token punctuation">(</span><span class="token parameter">userMessage</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>    <span class="token keyword">const</span> chatRequest <span class="token operator">=</span> <span class="token punctuation">{</span><br>        <span class="token literal-property property">model</span><span class="token operator">:</span> <span class="token function">getModelName</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>        <span class="token literal-property property">messages</span><span class="token operator">:</span> <span class="token punctuation">[</span><br>            <span class="token punctuation">{</span><br>                <span class="token literal-property property">role</span><span class="token operator">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span><br>                <span class="token literal-property property">content</span><span class="token operator">:</span> <span class="token string">"You are a helpful assistant."</span><br>            <span class="token punctuation">}</span><span class="token punctuation">,</span><br>            <span class="token punctuation">{</span><br>                <span class="token literal-property property">role</span><span class="token operator">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span><br>                <span class="token literal-property property">content</span><span class="token operator">:</span> userMessage<br>            <span class="token punctuation">}</span><br>        <span class="token punctuation">]</span><br>    <span class="token punctuation">}</span><span class="token punctuation">;</span><br>    <br>    <span class="token keyword">try</span> <span class="token punctuation">{</span><br>        <span class="token keyword">const</span> response <span class="token operator">=</span> <span class="token keyword">await</span> axios<span class="token punctuation">.</span><span class="token function">post</span><span class="token punctuation">(</span><br>            <span class="token function">getLLMEndpoint</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>            chatRequest<span class="token punctuation">,</span><br>            <span class="token punctuation">{</span><br>                <span class="token literal-property property">headers</span><span class="token operator">:</span> <span class="token punctuation">{</span> <span class="token string-property property">'Content-Type'</span><span class="token operator">:</span> <span class="token string">'application/json'</span> <span class="token punctuation">}</span><span class="token punctuation">,</span><br>                <span class="token literal-property property">timeout</span><span class="token operator">:</span> <span class="token number">30000</span> <span class="token comment">// 30 seconds</span><br>            <span class="token punctuation">}</span><br>        <span class="token punctuation">)</span><span class="token punctuation">;</span><br>        <br>        <span class="token keyword">if</span> <span class="token punctuation">(</span>response<span class="token punctuation">.</span>data <span class="token operator">&amp;&amp;</span> response<span class="token punctuation">.</span>data<span class="token punctuation">.</span>choices <span class="token operator">&amp;&amp;</span> response<span class="token punctuation">.</span>data<span class="token punctuation">.</span>choices<span class="token punctuation">.</span>length <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span><br>            <span class="token keyword">return</span> response<span class="token punctuation">.</span>data<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>message<span class="token punctuation">.</span>content<span class="token punctuation">.</span><span class="token function">trim</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><br>        <span class="token punctuation">}</span><br>        <br>        <span class="token keyword">throw</span> <span class="token keyword">new</span> <span class="token class-name">Error</span><span class="token punctuation">(</span><span class="token string">'No response choices returned from API'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><br>    <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span>error<span class="token punctuation">)</span> <span class="token punctuation">{</span><br>        <span class="token keyword">if</span> <span class="token punctuation">(</span>error<span class="token punctuation">.</span>response<span class="token punctuation">)</span> <span class="token punctuation">{</span><br>            <span class="token keyword">throw</span> <span class="token keyword">new</span> <span class="token class-name">Error</span><span class="token punctuation">(</span><span class="token template-string"><span class="token template-punctuation string">`</span><span class="token string">API returned status code </span><span class="token interpolation"><span class="token interpolation-punctuation punctuation">${</span>error<span class="token punctuation">.</span>response<span class="token punctuation">.</span>status<span class="token interpolation-punctuation punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token interpolation-punctuation punctuation">${</span><span class="token constant">JSON</span><span class="token punctuation">.</span><span class="token function">stringify</span><span class="token punctuation">(</span>error<span class="token punctuation">.</span>response<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token interpolation-punctuation punctuation">}</span></span><span class="token template-punctuation string">`</span></span><span class="token punctuation">)</span><span class="token punctuation">;</span><br>        <span class="token punctuation">}</span><br>        <span class="token keyword">throw</span> error<span class="token punctuation">;</span><br>    <span class="token punctuation">}</span><br><span class="token punctuation">}</span></code></pre><p>The above code defines an Express.js API endpoint that sends user messages to a Large Language Model (LLM), which will be Smollm2 in the example, and returns the model’s reply.</p><p>The <code>POST /api/chat</code> route reads a message from the request body. If the message is the special command <code>!modelinfo</code>, it immediately returns the current model name via <code>getModelName()</code>. Otherwise, it calls <code>callLLMAPI(message)</code> to get a response from the LLM. If anything fails, it logs the error and returns an HTTP 500 with a friendly error message.</p><p>The <code>callLLMAPI</code> function builds a request payload in a chat-style format: a system message that sets behavior (“You are a helpful assistant.”) and the user’s message. It sends this payload to the LLM endpoint (from <code>getLLMEndpoint()</code>) using Axios with a JSON header and a 30-second timeout.</p><p>If the API returns a valid response with choices, it extracts and trims the assistant’s reply. If no choices are returned or the API returns an error status, it throws a descriptive error so the caller can handle it properly. You can check the whole <a href="https://github.com/geshan/hello-genai/blob/smollm2-node-24/node-genai/app.js">app.js</a> and the <a href="https://github.com/geshan/hello-genai/blob/smollm2-node-24/node-genai/views/index.html">view</a> file too. In the next section, you will see the Dockerfile for this chat app.</p><h3 id="dockerfile-for-node-app" tabindex="-1">Dockerfile for Node app <a class="direct-link" href="#dockerfile-for-node-app">#</a></h3><p>The <code>Dockefile</code> below is used to run the Node.js chat app built with Express.js.</p><pre><code>FROM node:24-alpine

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm install

# Copy application code
COPY . .

# Create directories if they don't exist
RUN mkdir -p views

# Expose port 8080
EXPOSE 8080

# Run the application
CMD [&quot;node&quot;, &quot;app.js&quot;]
</code></pre><p>The above Dockerfile builds a lightweight container for a Node.js application. It would have been better to use a <a href="blog/2019/11/how-to-use-docker-multi-stage-build/">multistage Docker build</a>, still, it is fine for a demo app.</p><p>It starts from the <code>node:24-alpine</code> base image, which provides Node.js 24 on a small Alpine Linux distribution. <code>WORKDIR /app</code> sets <code>/app</code> as the working directory inside the container.</p><p><code>COPY package*.json ./</code> copies package.json and package-lock.json (if present) first, allowing Docker to cache dependency installation. <code>RUN npm install</code> installs all Node.js npm dependencies.</p><p><code>COPY . .</code> then copies the rest of the application source code into the container. <code>RUN mkdir -p views</code> ensures a <code>views</code> directory exists, preventing runtime errors if the app expects it.</p><p><code>EXPOSE 8080</code> documents that the app listens on port 8080. Finally, <code>CMD [&quot;node&quot;, &quot;app.js&quot;]</code> defines the default command to start the application when the container runs.</p><p>It is a good enough Dockefile for a small chat app like this. The next section details the Docker Compose file that links this app and container to the Smollm2 model.</p><h3 id="docker-compose-file-with-node-app-and-smollm2-model" tabindex="-1">Docker compose file with Node app and Smollm2 model <a class="direct-link" href="#docker-compose-file-with-node-app-and-smollm2-model">#</a></h3><p>You can link up the Node.js Chat app with the Smollm2 model easily using a Docker Compose file that looks like:</p><pre class="language-bash"><code class="language-bash"><span class="highlight-line">services:</span><br><span class="highlight-line">  node-genai:</span><br><span class="highlight-line">    build:</span><br><span class="highlight-line">      context: <span class="token builtin class-name">.</span></span><br><span class="highlight-line">      dockerfile: Dockerfile</span><br><span class="highlight-line">    ports:</span><br><span class="highlight-line">      - <span class="token string">"8082:8080"</span></span><br><span class="highlight-line">    environment:</span><br><span class="highlight-line">      - <span class="token assign-left variable">PORT</span><span class="token operator">=</span><span class="token number">8080</span></span><br><mark class="highlight-line highlight-line-active">    models:</mark><br><mark class="highlight-line highlight-line-active">      - smollm2</mark><br><span class="highlight-line">    healthcheck:</span><br><span class="highlight-line">      test: <span class="token punctuation">[</span><span class="token string">"CMD"</span>, <span class="token string">"curl"</span>, <span class="token string">"-f"</span>, <span class="token string">"http://localhost:8080/health"</span><span class="token punctuation">]</span></span><br><span class="highlight-line">      interval: 30s</span><br><span class="highlight-line">      timeout: 10s</span><br><span class="highlight-line">      retries: <span class="token number">3</span></span><br><span class="highlight-line">      start_period: 10s</span><br><span class="highlight-line"></span><br><mark class="highlight-line highlight-line-active">models:</mark><br><mark class="highlight-line highlight-line-active">  smollm2:</mark><br><mark class="highlight-line highlight-line-active">    model: ai/smollm2</mark><br><mark class="highlight-line highlight-line-active">    context_size: <span class="token number">8048</span></mark></code></pre><p>The above Docker Compose file defines a single service, <code>node-genai</code>, and its related AI model configuration.</p><p>The <code>node-genai</code> service is built from the local directory using the specified <code>Dockerfile</code> as seen in the above section. It maps port <code>8082</code> on the host to port <code>8080</code> inside the container, allowing external access to the Node.js app. The <code>PORT=8080</code> environment variable configures the app’s listening port. The models section links this service to a model named <code>smollm2</code>.</p><p>A health check is a mechanism for verifying that the service is running correctly. It uses curl to call <a href="http://localhost:8080/health">http://localhost:8080/health</a> every 30 seconds, with a 10-second timeout, three retries, and a 10-second startup grace period.</p><p>The models section declares the Smollm2 model, referencing the image ai/smollm2 from DockerHub and setting a context size of 8048 tokens, which controls how much text the model considers at once.</p><p>In the next section, you will run the chat app and the model together with Docker Compose.</p><h3 id="running-the-app-with-docker-compose" tabindex="-1">Running the app with Docker Compose <a class="direct-link" href="#running-the-app-with-docker-compose">#</a></h3><p>To run the Node.js Chat app built with Express.js and Smollm2 attached to it, you can run the following command, which will first build the app and then run the app and the model in the background:</p><pre class="language-bash"><code class="language-bash"><span class="token function">docker</span> compose build <span class="token operator">&amp;&amp;</span> <span class="token function">docker</span> compose up <span class="token parameter variable">-d</span></code></pre><p>It will give an output like the following:</p><img class="center" src="/images/docker-model-runner-docker-compose/03docker-model-runner-build-up.jpg" loading="lazy" title="Docker Model runner and Docker compose to build and run a Node.js App with Smollm2 open model" alt="Docker Model runner and Docker compose to build and run a Node.js App with Smollm2 open model"><p>You don’t need to build the container every time; you can do <code>docker compose up -d</code> the next time. After that, to confirm that the containers are runnin,g you can execute:</p><pre class="language-bash"><code class="language-bash"><span class="token function">docker</span> compose logs <span class="token parameter variable">-f</span></code></pre><p>Which will show something like:</p><img class="center" src="/images/docker-model-runner-docker-compose/04docker-model-runner-compose-logs.jpg" loading="lazy" title="Docker Model runner and Docker compose logs of Node.js App with Smollm2 open model" alt="Docker Model runner and Docker compose logs of Node.js App with Smollm2 open model"><p>As the app is running on localhost port <code>8082</code>, you can open your browser of choice and hit <code>http://localhost:8082</code>, which will render something similar to:</p><img class="center" src="/images/docker-model-runner-docker-compose/05dmr-nodejs-smollm2-app-running.jpg" loading="lazy" title="Docker Model runner and Docker compose Node.js App with Smollm2 open model running on the browser" alt="Docker Model runner and Docker compose Node.js App with Smollm2 open model running on the browser"><p>Docker includes an internal URL for the Model’s APIs at <code>http://model-runner.docker.internal/</code>, which can be called by other containers. If you want to call it from localhost, it is running on port <code>12434</code>. Then you can chat with Smolllm2 via the simple Node.js chat interface.</p><p>To check the running containers, you can run <code>docker compose ps</code>. To stop the containers, run <code>docker compose stop</code>. To check the running models, you can run <code>docker model ps</code>. To unload Smollm2, you can run <code>docker model unload smollm2</code>; if you run <code>docker model ps</code>, it will not be running Smollm2 anymore.</p><p>In my experience, when running AI models locally, they can start to eat up CPU resources. So it is best to unload the model when you are done using it.</p><h2 id="conclusion" tabindex="-1">Conclusion <a class="direct-link" href="#conclusion">#</a></h2><p>In this post, you built on part 1, the introduction to Docker Model Runner, by connecting a simple Node.js (Express.js) chat app to the Smollm2 model using Docker Compose. You saw the code for the chat app, then a simple Dockerfile to run a Node.js app, and finally the Docker Compose file that connects the app to the open model (SmolM2 in this case). Then you ran it and saw it in action. Keep Learning!</p></div><div class="flex items-center justify-between gap-3 md:gap-4 flex-wrap md:flex-nowrap py-2 border-y border-lightbg mt-4"><div class="flex items-center justify-start md:gap-4 gap-2 flex-wrap w-2/3"><div class="flex justify-start items-center gap-3 md:gap-4"><img src="/images/favicons/favicon-32x32-pp.png" alt="Geshan Manandhar" class="w-10 h-10 rounded-full object-cover flex-shrink-0"> <span class="text-gray font-medium font-ui text-md md:text-xl uppercase tracking-wide text-start">Geshan Manandhar</span></div><time datetime="2026-01-25" data-updated="true"><time class="entry-date" datetime="2026-01-25"><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">25-Jan-2026 </span></time></time><span class="text-textColor font-ui font-medium text-md md:text-xl uppercase">12 MIN READ</span></div><button type="button" id="share-button-bottom" class="flex items-center justify-center w-8 h-8 md:w-9 md:h-9 rounded text-lightGray hover:border-gray-400 hover:bg-gray-50 transition-colors flex-shrink-0" aria-label="Share this post" onclick="toggleShareIconsBottom(); ga('send', 'event', 'share', 'clickShareOnPost');"><img src="/images/shareIcon.svg" alt="Share this post" class="w-6 h-6"></button></div><div id="share-icons-container-bottom" class="share-icons-bottom print:hidden hidden mx-auto" aria-label="Share this post"><div class="w-full flex items-center justify-center mx-auto gap-3"><span class="share-icons"><a href="https://www.linkedin.com/shareArticle?mini=true&url=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on LinkedIn" onclick="ga('send', 'event', 'share', 'clickShareOnLinkedIn');"><span id="linkedin-share" class="text-[#0077B5]"><img src="/images/social-icons/linkedin.svg" alt="LinkedIn" class="sharing-common"> </span></a><a href="http://twitter.com/share?text=How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2] - &url=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on Twitter" onclick="ga('send', 'event', 'share', 'clickShareOnTwitter');"><span id="twitter-share" class="text-[#1DA1F2]"><img src="/images/social-icons/twitter.svg" alt="Twitter" class="sharing-common"> </span></a><a href="http://www.facebook.com/sharer.php?u=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on Facebook" onclick="ga('send', 'event', 'share', 'clickShareOnFacebook');"><span id="facebook-share" class="text-[#1877F2]"><img src="/images/social-icons/facebook.svg" alt="Facebook" class="sharing-common"> </span></a><a href="https://t.me/share/url?url=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on Telegram" onclick="ga('send', 'event', 'share', 'clickShareOnTelegram');"><span id="telegram-share" class="text-[#0088cc]"><img src="/images/social-icons/telegram.svg" alt="Telegram" class="sharing-common"> </span></a><a href="https://api.whatsapp.com/send?text=https://geshan.com.np/blog/2026/01/docker-model-runner-docker-compose/" target="_blank" title="Share 'How to use an open model with your application using Docker Model Runner and Docker Compose [Part 2]' on Whatsapp" onclick="ga('send', 'event', 'share', 'clickShareOnWhatsapp');"><span id="whatsapp-share" class="text-[#25d366]"><img src="/images/social-icons/whatsapp.svg" alt="WhatsApp" class="sharing-common"></span></a></span></div></div></article><div class="mt-3"><a href="/blog/categories/ai/" title="AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">ai </a><a href="/blog/categories/gen-ai/" title="Gen AI" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">gen ai </a><a href="/blog/categories/docker/" title="Docker" class="background-lightbg mr-4 mb-2 inline-block mt-2 rounded border-2 bg-gray-100 font-semibold font-ui text-gray hover:text-gray-400 px-3 py-1 text-xl border-gray-300 cursor-pointer hover:border-gray-400">docker</a></div><section class="md:w-full mx-auto mt-4"><h3 class="comments font-heading">Comments</h3><div id="disqus_thread"></div><button id="disqus_trigger" class="font-ui text-blackText hover:border-darkAvocado border-blacktext hover:shadow-lg border-2 transition ease-in-out delay-100 px-3 py-2 pt-2 mt-2 rounded-md text-center block h-auto text-xl" onclick="load_disqus()">Post a Comment</button><div id="disqus_thread" aria-live="polite"></div></section></div></div><section class="md:w-full mx-auto mt-4 bg-lightAvocado p-4 rounded-lg pb-8"><div class="max-w-6xl mx-auto md:py-12 px-4"><h2 class="text-4xl text-darkAvocado font-heading font-regular flex-shrink-0 py-8 tracking-normal leading-[105%] md:leading-[105%]"><span class="text-lightGray italic">Related</span> <span class="text-blackText">Blogs</span></h2><div class="grid grid-cols-1 md:grid-cols-2 gap-8"><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2026/01/docker-model-runner/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">Docker Model Runner: A beginner’s guide to running open models on your own machine [Part 1]</a> <time class="font-medium text-xl text-textColor uppercase font-ui">23-Jan-2026 &nbsp;&nbsp;&nbsp; 6 min read</time><p class="text-xl text-gray font-ui">A tutorial showing how to run Smollm2 with Docker Model Runner mainly with the Docker Model CLI</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/12/gemini-live-audio/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to use Gemini Live audio as an interviewer for a software engineer’s job (with video)</a> <time class="font-medium text-xl text-textColor uppercase font-ui">16-Dec-2025 &nbsp;&nbsp;&nbsp; 7 min read</time><p class="text-xl text-gray font-ui">Learn how to use Gemini Live audio as an interviewer for a software engineer’s job with this practical guide. Use Gemini as a interviwer for a backend engineer role and talk with it</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/11/gemini-3-ai/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to create a hair style changer app using Gemini 3 on Google AI Studio</a> <time class="font-medium text-xl text-textColor uppercase font-ui">27-Nov-2025 &nbsp;&nbsp;&nbsp; 7 min read</time><p class="text-xl text-gray font-ui">Learn how to create a hair style changer app using Gemini 3 on Google AI Studio with this practical guide. Explore its features and find out how to build your own app.</p></div><div class="py-4 border-b border-avocado pb-4 flex flex-col gap-2 font-ui"><a href="/blog/2025/11/how-to-use-notebooklm/" class="font-semibold font-ui text-gray text-xl md:text-2xl tracking-[-0.01em] leading-[105%] md:leading-[105%]">How to use NotebookLM: A practical guide with examples</a> <time class="font-medium text-xl text-textColor uppercase font-ui">23-Nov-2025 &nbsp;&nbsp;&nbsp; 15 min read</time><p class="text-xl text-gray font-ui">Learn how to use Google NotebookLM effectively with this practical guide. Explore its features and find out how to use it for Job search with practical examples.</p></div></div></div></section><section class="bg-avocado md:py-0 py-12 px-4"><div class="max-w-6xl mx-auto"><div class="md:hidden"><h2 class="mb-4 text-3xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-base text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-base text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full text-center">Follow on LinkedIn &nbsp;→</a></div><div class="hidden md:grid lg:hidden grid-cols-3 gap-8 items-center"><div class="z-10 col-span-2"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity">Follow on LinkedIn &nbsp;→</a></div><div class="flex items-center justify-end col-span-1"><img class="w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"></div></div><div class="hidden lg:grid grid-cols-2 gap-8 lg:gap-12 items-center justify-between h-full"><div class="z-10"><h2 class="mb-4 text-2xl md:text-4xl font-heading font-regular text-textColor break-words tracking-[-0.01em] leading-[105%] md:leading-[105%]"><span class="text-textColor italic">Stay</span><span class="text-blackText"> Connected</span></h2><p class="text-xl text-blackText font-body font-regular mb-6">Follow me on LinkedIn for new posts, engineering insights, and tech takes — straight from the trenches.</p><p class="text-xl text-blackText font-body font-regular mb-6">A big thank you for supporting this ad. free, unobstructive (no overlays, no pop-ups) blog.</p></div><div class="flex items-center gap-12 w-full md:w-auto justify-end self-end"><img class="w-16 md:w-32 h-auto" src="/images/theme/new_logo.svg" alt="Geshan"> <a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer" class="inline-block bg-darkAvocado text-xl text-white px-6 py-3 rounded-lg font-body font-regular hover:opacity-90 transition-opacity w-full md:w-auto text-center md:text-left">Follow on LinkedIn &nbsp;→</a></div></div></div></section></div></div></div></main><footer role="contentinfo"><footer class="bg-avocado sm:px-4"><div class="max-w-6xl mx-auto flex flex-col md:flex-row sm:flex-col justify-between gap-2 items-center sm:items-center md:items-center py-10 font-nav text-lightGray text-base font-regular"><div class="pb-0 sm:pb-2 text-gray"><span class="pr-4">Copyright © 2026 Geshan Manandhar.</span></div><div class="pb-0 sm:pb-2"><ul class="flex"><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.linkedin.com/in/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> LinkedIn</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://www.twitter.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Twitter</a></li><li class="hover:text-gray-300 text-lg pr-4 cursor-pointer text-gray"><a href="https://github.com/geshan" target="_blank" rel="noopener noreferrer nofollow" class="flex items-center gap-2"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 1000 1000"><path fill="currentColor" d="M1000 267q0 112-78 188L747 631q-78 78-189 78q-97 0-171-63l115-115q26 17 56 17q44 0 75-31l175-176q31-29 31-74q0-44-30.5-74.5T734 162q-25 0-49.5 12T652 208H414L546 79Q626 1 734 1q110 0 188 78t78 188zm-387 89L498 471q-26-17-56-17q-44 0-75 31L192 661q-31 29-31 74q0 44 30.5 74.5T266 840q25 0 49.5-12t32.5-34h238L454 923q-80 78-188 78q-110 0-188-78T0 735q0-112 78-188l175-176q78-78 189-78q97 0 171 63z"/></svg> Github</a></li></ul></div></div></footer></footer><script src="/js/all.min.js" defer="defer"></script></body></html>